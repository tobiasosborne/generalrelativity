# General Relativity, Lecture 5: flows and tensors
# YouTube: https://www.youtube.com/watch?v=4MzK6BVuwHo
# Auto-generated transcript

[00:00.32] hello and welcome back to introduction
[00:01.76] to general relativity
[00:03.12] in the previous lecture we looked at
[00:04.56] tangent space which is a way of
[00:06.00] associating
[00:06.80] vector spaces to manifolds in today's
[00:09.36] lecture we'll take a look at flows on
[00:11.04] manifolds and then at tensors which is a
[00:13.84] subject that will play a very important
[00:16.00] role throughout our studies of general
[00:17.44] relativity
[00:18.96] so if you recall we had the notion of a
[00:21.68] tangent vector field in the previous
[00:23.12] lecture and today we're going to
[00:24.16] associate the tangent vector fields
[00:26.64] flows of maps generated by these vector
[00:29.44] fields and vice versa but to talk about
[00:31.44] that first we need to introduce the
[00:32.88] notion
[00:33.44] of a one parameter group of
[00:35.48] diffiomorphisms
[00:37.12] so as the name suggests a one parameter
[00:39.52] group of diphymorphisms is a bunch of
[00:41.36] diffumorphisms
[00:42.96] so let's take a look at the actual
[00:44.32] definition let m be a manifold
[00:46.56] a one parameter group of diffumorphisms
[00:48.88] phi t is a c
[00:50.00] infinity map from r across m to m so m
[00:53.68] is a manifold and r is like time
[00:55.84] and so phi of t is indexed by some time
[00:58.64] parameter
[00:59.28] and it gives you a map
[01:02.64] from r cross m from m to m for a given t
[01:07.12] so it's not just these groups of
[01:09.76] diphymorphisms are have additional
[01:12.48] properties right
[01:14.08] they also need to satisfy a kind of
[01:16.72] composition property
[01:18.40] so for a fixed time t
[01:20.54] [Music]
[01:22.24] real real time t the map phi t has to be
[01:25.12] well as the name suggests
[01:26.56] a diffumorphism and for all
[01:29.60] choices of s and t so you can choose uh
[01:32.40] two different times
[01:33.76] when you take phi t and then compose it
[01:35.68] with phi s you get five t
[01:37.36] plus s this is the group property
[01:40.56] thus if you choose
[01:44.72] s equals minus t
[01:48.24] you can see from the this property above
[01:51.68] this property that i'm about to
[01:52.80] highlight here you can see that this
[01:54.72] property implies already
[01:56.32] that phi of t equals naught is the
[01:58.96] identity map
[02:04.48] if you don't know what a group is then
[02:06.56] do take the uh
[02:08.16] opportunity right now to go ahead and
[02:10.08] google what is a mathematical group
[02:14.48] so given such a group of of
[02:16.52] diffiomorphisms
[02:17.92] you can think of them as like evolutions
[02:19.60] for some dynamical system on the
[02:21.28] manifold
[02:22.16] you can obtain a tangent vector field so
[02:25.28] we can go from one parameter groups of
[02:27.28] diffumorphisms to tangent vector fields
[02:29.28] and i'll show you how to do that now
[02:34.84] so
[02:36.40] given such a phi t
[02:38.75] [Music]
[02:42.88] we can obtain a tangent vector field
[02:46.64] as follows
[02:58.88] so we fix a point p in the manifold so
[03:01.52] here's my cartoon of the manifold
[03:03.44] m here's a point p
[03:06.64] in the manifold and we fix it and now we
[03:09.68] apply
[03:10.24] a one parameter group of diffumorphism
[03:12.08] to that point and that'll
[03:13.52] push this point around the manifold on
[03:15.60] some trajectory
[03:20.08] so for fixed p
[03:26.80] we can take a look at
[03:30.16] the following subset of m
[03:35.92] phi t of p so phi t goes from r
[03:39.68] to m and it is actually a curve
[03:44.72] according to the definition curves i
[03:46.32] gave in the previous lectures
[03:48.00] it's a curve c drawn in the picture here
[03:51.52] and it's called the orbit
[03:57.68] of p under phi t
[04:03.12] and now given a curve we're able to
[04:05.44] define a tangent vector right i showed
[04:07.44] you how to do that in a previous lecture
[04:09.92] we can define the tangent vector v at p
[04:12.56] to be
[04:14.80] none other than the tangent vector to
[04:17.20] this curve
[04:25.76] at t equals zero so that's how we can
[04:29.36] associate a tangent vector
[04:31.20] to a there's the tangent vector i've
[04:35.36] drawn a cartoon of the tangent vector
[04:37.04] there
[04:38.08] uh v at p
[04:42.08] and of course now all you have to do is
[04:45.04] vary
[04:45.92] p over the manifold and you'll get a
[04:47.60] tangent vector for every point in the
[04:48.96] manifold
[04:50.08] according to this recipe now
[04:53.20] interestingly there's a converse to this
[04:54.88] result
[04:58.56] you know we've gone from one parameter
[04:59.92] group of diffumorphisms to
[05:01.68] tension vector fields now we're going to
[05:02.96] go from tangent vector fields to one
[05:04.24] parameter groups of diffumorphisms
[05:08.84] so let v
[05:11.36] be a smooth vector field so it's got to
[05:14.72] be nice and differentiable
[05:21.60] on m we're going to now try and find
[05:29.20] one parameter group of diffumorphisms
[05:31.04] associated to this tangent vector field
[05:35.68] and these one parameter groups of
[05:36.96] diffumorphisms have a name they call the
[05:38.72] integral curves
[05:51.28] and the way we do that well what is an
[05:53.68] integral curve first of all
[05:55.28] an integral curve is a family of curves
[05:58.08] on m
[05:58.56] having the property that uh exactly one
[06:01.28] curve passes through
[06:02.48] each point in m and at that point uh the
[06:05.52] tangent vector to the curve is this
[06:07.28] given v
[06:08.00] of p so it's
[06:11.28] put a colon here what is a
[06:15.20] what are integral curves it's a family
[06:16.88] of curves
[06:19.76] in m there's uh
[06:23.28] exactly
[06:26.96] one curve through each point
[06:38.64] and the tangent at each point p in m
[06:41.60] let's say
[06:44.56] and each
[06:48.16] and and the tangent vector
[07:00.24] to this curve
[07:03.52] is none other than just v at the point p
[07:09.20] so there is this converse i claim it's
[07:11.04] true i'm not going to prove it in this
[07:12.64] course
[07:13.12] that's actually fairly involved but i
[07:16.00] will show you
[07:18.00] how to build up the equations that you
[07:19.92] need to solve
[07:21.12] and then uh point you in the direction
[07:23.52] of where to look for
[07:24.40] the general answer so how are we going
[07:26.72] to do this well first of all we're going
[07:28.08] to just write out the two definitions we
[07:29.76] have
[07:30.16] of the tangent vector right so we've got
[07:32.96] the tangent vector
[07:34.16] what is it it's a function a tangent
[07:35.68] vector t is a function t
[07:37.52] that eats a function f on the manifold
[07:39.60] and gives us a number
[07:41.60] and we can get a tangent vector to a
[07:44.56] curve according to the following recipe
[07:47.44] so we can take our function f we can
[07:49.28] compose it with the curve
[07:50.64] and then differentiate it with respect
[07:52.00] to this this time parameter t
[07:54.56] and then when you write that out into
[07:56.16] components which we've done in previous
[07:58.32] lectures then you see that you can
[08:01.68] write it in terms of the coordinate
[08:03.04] vector fields big x mu
[08:05.28] applied to f and now we've got these
[08:07.84] coefficient functions dx mu
[08:09.84] dt so this is in coordinates the the
[08:11.84] condition for t to be tangent to the
[08:14.08] curve c
[08:16.24] so the curve c here c is the curve
[08:21.84] the curve we're solving for right we
[08:23.28] don't know it's there yet so
[08:36.40] so we are solving for this
[08:41.52] there's a second way uh to
[08:44.64] have a tangent vector to point p and
[08:46.80] that is we can just simply
[08:48.40] say it has to be at a point the tangent
[08:51.60] vector to point p
[08:52.40] is the tangent vector at the point p so
[08:54.72] we we have our given tangent vector v
[08:57.52] at the point p that we want this curve
[08:59.84] to be integral for
[09:02.00] and if you write that out in components
[09:05.20] right so v is an element of a tangent
[09:07.28] space we can always write it in terms of
[09:08.88] a coordinate basis
[09:11.36] then you get that expression now for the
[09:13.44] tangent vector
[09:14.88] at the point p acting on a function
[09:17.04] smooth function and of course
[09:18.48] you know just using the different
[09:19.52] notations here
[09:21.68] that's these big x muse i'd like to use
[09:24.80] different notations just to
[09:26.24] remind you that there are so many
[09:27.68] different notations and then they are
[09:29.12] actually equivalent
[09:31.04] so we've got these two two uh
[09:33.12] expressions
[09:34.24] for a tangent vector at a point p one
[09:35.92] associated to a curve one
[09:37.44] is the one that's given at the moment v
[09:39.36] is given
[09:40.48] so we must solve to find
[09:44.24] to find c the integral curve we equate
[09:48.72] one and two so we set one equals two
[09:52.96] um and so we must solve
[09:59.04] the following equations
[10:02.88] it's a set of nonlinear differential
[10:05.68] equations for these coordinates x
[10:16.84] mu
[10:19.52] so that's the equations we have to solve
[10:22.00] in order
[10:24.00] to find a set of integral curves
[10:34.00] this is a set of nonlinear ordinary
[10:36.88] differential equations
[10:44.72] now such a system of coupled so coupled
[10:48.72] ordinary differential equations
[10:55.28] now given such a set of coupled ordinary
[10:57.68] differential equations there actually is
[11:01.36] a unique solution
[11:06.96] at least for some sufficiently small
[11:12.84] time
[11:17.28] that means there exists an epsilon
[11:18.96] greater than zero
[11:21.20] uh such that for all t inside so if
[11:24.80] as long as t is sufficiently small there
[11:27.12] really is a unique solution
[11:41.36] so we'll say it like this there is a
[11:42.88] unique solution um as long as
[11:44.88] there exists an epsilon greater than
[11:46.24] zero and as long as t is less than minus
[11:48.40] epsilon
[11:49.04] bigger than minus epsilon less than
[11:50.40] epsilon there is a unique solution to
[11:52.16] such a set of non-linear differential
[11:53.68] equations
[11:54.72] this is uh this result takes some
[11:56.64] proving uses some
[11:58.00] fixed point arguments um it's definitely
[12:00.56] outside the scope of this course today
[12:03.04] and i just refer you to the textbook say
[12:05.04] of codington and levinson
[12:06.64] uh to if you're really interested in
[12:08.80] taking a look at the details of that
[12:10.24] argument
[12:11.44] and but you can find it in no in
[12:14.88] no shortage of places there's lots and
[12:16.32] lots of places where you can find this
[12:17.68] result
[12:18.96] if you're interested to follow it up
[12:21.12] just look for existence and uniqueness
[12:22.80] and nonlinear ordinary differential
[12:24.16] equations is the search phrase i guess
[12:26.80] it's worth saying that if m is compact
[12:29.04] as a manifold
[12:32.16] then we can say more
[12:37.52] existence can sometimes be for all tea
[12:39.84] on compact manifolds
[12:44.40] okay so that concludes really the
[12:45.84] discussion of uh
[12:48.64] what we might call elementary
[12:51.92] relativistic mechanics on manifolds or
[12:54.08] elementary mechanics on manifolds is not
[12:56.48] yet relativistic right we don't have any
[12:58.48] uh any special relativity happening yet
[13:01.28] or any general relativity we just have
[13:02.88] manifolds
[13:05.28] and now i want to turn to
[13:08.48] another topic to do with tangent spaces
[13:10.64] an extremely important topic
[13:19.20] namely tenses
[13:24.24] actually before i do that i want to
[13:26.80] spend a couple of moments
[13:28.16] talking before we get to actual tenses i
[13:30.48] want to also
[13:31.36] describe to you a construction whereby
[13:33.76] we can given two vector fields build a
[13:35.92] new vector field
[13:37.52] so let me just back up for a moment u
[13:39.76] vector fields
[13:42.80] from old so this is a very important
[13:44.72] topic when you're building something you
[13:46.48] should always
[13:47.44] understand how you can use your existing
[13:50.56] objects to construct new objects
[13:52.64] so in this at this point i'm now talking
[13:56.00] about vector fields
[14:01.36] so that v and w be smooth vector fields
[14:11.12] on a manifold m
[14:16.00] then it turns out that given two vector
[14:17.68] fields we can define a third one
[14:19.68] by a special construction called the
[14:21.12] commutator
[14:26.24] let's i don't know what should we call
[14:27.76] it u
[14:31.44] and u this new vector field
[14:35.36] has a has a special notation for it but
[14:37.12] i write it as u first just to emphasize
[14:38.88] that it's just a vector field and
[14:40.16] nothing
[14:40.64] nothing else how do you construct this
[14:42.96] new vector field given two other vector
[14:44.64] fields well you can apply
[14:46.88] w first to a function f so f is a smooth
[14:50.48] function so for all f in the smooth
[14:52.32] functions on the manifold
[14:54.24] and you get a number so there's not a
[14:55.84] vector field is it but then um
[15:00.56] so you got u u is defined its action the
[15:03.52] u
[15:03.84] the action of u on f is defined to be w
[15:06.08] first on f
[15:07.28] then you do v on this smooth
[15:10.32] function w of f it's also a number
[15:14.00] you could do that but you could have
[15:15.12] done it the other way right
[15:19.36] and so this prescription here defines a
[15:22.80] new vector field
[15:24.16] you can check that that does indeed give
[15:25.68] a new vector field
[15:28.08] and this vector field has a name it's
[15:29.60] called a commutator
[15:33.52] it turns out to play an important role
[15:34.88] in the theory of lead groups
[15:38.00] so this is a vector field
[15:45.44] and that's a little exercise for you you
[15:46.96] just have to check that it is a map from
[15:48.96] this mini functions on the m
[15:51.84] to the numbers and it's called the
[15:54.16] commutator
[15:59.92] of v and w and we have a
[16:03.04] an alternative notation that you'll see
[16:04.72] used almost everywhere
[16:06.84] so we also denote
[16:09.52] u to be v comma w in square brackets
[16:13.44] it's another notation
[16:20.32] okay so that's the topic of getting new
[16:21.92] vector fields from old vector fields now
[16:24.40] let's get
[16:26.08] start our discussion of the wider topic
[16:27.92] of new vector spaces on manifolds from
[16:34.84] old
[16:42.00] okay it's worth saying that you know
[16:43.60] we've got a way of associating a vector
[16:46.08] space or a family of vector spaces with
[16:47.84] a manifold m
[16:49.28] given any point now in a manifold m we
[16:51.68] can build we can associate a natural
[16:53.44] vector space with it called vp the
[16:55.28] tangent space
[16:56.56] to the manifold at the point p
[16:59.68] once you've got a vector space you can
[17:03.44] build other vector spaces and this is
[17:06.08] well
[17:06.48] you could see this is a somewhat baroque
[17:08.80] process where given one
[17:10.88] vector space you build arbitrarily
[17:12.72] complicated other vector spaces via
[17:14.80] elementary operations from it
[17:16.72] now this is only useful if you in
[17:18.56] physics if you've got a reason to
[17:20.00] introduce these vector spaces and it
[17:21.60] turns out we do right there are
[17:22.80] quantities in
[17:23.84] physics especially in general relativity
[17:26.56] which are naturally associated to
[17:29.84] vector space uh tensor products of
[17:31.60] vector spaces is
[17:33.12] the the the main source of these these
[17:35.04] quantities and we're going to meet these
[17:37.20] in the following lectures
[17:38.32] so the the most important quantity of
[17:40.08] course uh coming up is that of the
[17:41.76] metric
[17:42.24] the metric is a thing that lives not in
[17:44.96] the tangent vector space but actually it
[17:46.64] naturally lives in a tensor product of
[17:48.16] two vector spaces
[17:49.84] another vector space that plays an
[17:51.76] increasingly important role throughout
[17:53.12] physics is out of the dual vector space
[17:54.88] we're going to see that
[17:56.00] as i am coming up quite often
[17:59.68] especially in our discussion of metrics
[18:02.00] we'll also want to talk about
[18:03.84] more general tensor fields coming out of
[18:06.16] uh
[18:07.12] associated with manifolds and these will
[18:08.72] require that we take up to four fold
[18:10.32] tensor products of vector spaces
[18:12.64] so i'm going to have to tell you about
[18:13.92] some of these operations
[18:17.04] and we're going to use these operations
[18:19.92] to build all kinds of vector spaces now
[18:21.60] i should say right that you know just
[18:23.04] because we have these operations doesn't
[18:24.56] mean
[18:25.20] we need them in physics i am going to
[18:27.04] tell you uh i'm going to list at least
[18:29.04] the operations that we can introduce to
[18:30.72] build new vector spaces
[18:32.48] but we're only going to use really a
[18:33.84] subset of these to build the quantities
[18:35.68] that are interesting in physics so just
[18:37.04] because the operations are there doesn't
[18:38.48] mean it doesn't mean you need them in
[18:40.00] physics
[18:41.60] you should assess the the value of a
[18:44.56] definition in physics by whether or not
[18:46.24] it gives you quantities
[18:47.36] or allows you to express quantities that
[18:48.88] have a physical interpretation
[18:51.76] so what i want to do for the next couple
[18:53.20] of minutes is just list ways in which
[18:54.96] you can take a vector space or
[18:56.72] other vector spaces and build new vector
[18:58.48] spaces this is really a uh
[19:00.80] in mathematics of course this plays a
[19:02.24] fundamental role in understanding the
[19:04.48] category of all types of all objects of
[19:06.96] one type so
[19:07.76] you could think of the category of
[19:08.80] vector spaces it's really important to
[19:10.72] understand what operations take a thing
[19:12.48] and give you a new thing a new instance
[19:14.08] of that thing
[19:15.20] here well i'll we'll take a slightly
[19:17.76] more example driven point of view
[19:20.08] so let's list now the kinds of
[19:22.08] operations that we can use to get new
[19:23.68] vector spaces from old vector spaces
[19:26.08] so we could take so-called duals so
[19:29.12] i'll explain what this is in a minute
[19:31.20] you know given one vector space we can
[19:32.48] get another one by just
[19:34.00] building the vector space of all
[19:35.12] functions on it i'll define that
[19:36.84] presently
[19:38.24] we that gives us a new vector space
[19:41.28] so we can take direct sums given two
[19:44.72] vector spaces
[19:48.64] v and w we can form a new vector space
[19:53.20] by taking their direct sum
[19:54.80] it's one way of building a new vector
[19:56.24] space we can take tensor products
[20:02.00] don't worry i'm going to explain define
[20:03.84] these these words so given
[20:06.08] v and w you can form a new vector space
[20:07.92] called v tensor w
[20:09.84] uh there's a couple of other operations
[20:11.60] that we could use to build new vector
[20:13.52] spaces
[20:14.40] we could take subspaces
[20:18.88] so given a vector space v and some
[20:21.28] subspace u of it
[20:23.60] uh that gives us a new vector space
[20:25.84] right if you have some vector space you
[20:27.28] take a subspace
[20:28.16] well u is a new subspace
[20:31.20] we could take intersections
[20:37.52] so if you've got u you've got v then you
[20:39.52] can form a new vector space which is
[20:40.96] just the intersection of u
[20:42.64] and v it's not so commonly seen
[20:46.08] although you know in quantum information
[20:48.16] theory these sort of things turn up
[20:49.92] and we can take joins of vector spaces
[20:56.48] also a somewhat uncommon operation uh in
[20:59.76] in physics why not i've just written the
[21:01.12] wrong notation but
[21:02.00] we could so i'll go back to the
[21:03.44] intersections here i wrote the wrong
[21:05.20] thing so take intersections if you've
[21:06.64] got two vector spaces v and w
[21:08.64] you can form the intersection which is v
[21:11.60] whatever you call that
[21:12.64] and symbol w and you can also take joins
[21:15.84] so given two vector spaces v and w
[21:17.92] you can take v kind of union w
[21:21.04] what is this vector space it's the
[21:22.32] smallest vector space which contains v
[21:24.24] and w as the subspace
[21:27.60] and you know there's probably more
[21:30.64] operations
[21:31.76] that i've forgotten so just given this
[21:34.88] list of
[21:35.36] six uh operations you can form a vast
[21:38.88] variety of interesting vector spaces
[21:41.04] given just one starting vector space
[21:44.32] and we're going to do that like you know
[21:45.84] general relativity is certainly no
[21:47.20] exception we're going to need lots of
[21:48.48] vector spaces we have one basic
[21:50.40] atomic vector space that we have
[21:52.00] canonically given to us namely the
[21:53.60] tangent vector space
[21:54.64] and we're going to use that with in
[21:56.48] combination with all these operations to
[21:58.24] build all kinds of other vector spaces
[22:00.24] the ones that will play an especially
[22:01.92] important role in general relativity is
[22:03.44] taking joules definitely
[22:05.36] and also tensor products will play
[22:07.84] incredibly important role in general
[22:09.44] relativity
[22:10.80] uh taking some spaces will play a minor
[22:13.44] role but
[22:14.00] somewhat important role direct sums
[22:15.60] doesn't play such a role uh
[22:17.36] in in uh general relativity so much but
[22:21.28] it does in the theory of fiber bundles
[22:22.80] so we're not really going to look at
[22:24.32] taking direct sums in this in this
[22:26.72] course
[22:27.84] taking intersections and joins plays as
[22:30.40] i said an important role in quantum
[22:31.76] information theory but not such an
[22:33.44] important role in quantum
[22:35.36] in general relativity so we'll i will
[22:37.76] won't talk about that any longer
[22:41.12] so the the objective for the rest of the
[22:43.92] lecture
[22:44.40] now is to just define for you uh
[22:47.60] the procedure of taking a joule of a
[22:49.52] vector space and
[22:50.64] the notion of a tensor product of two
[22:52.40] vector spaces
[22:54.80] so let's turn to that now joules
[22:59.28] so here's a definition
[23:03.60] write it differently
[23:10.64] so let v be a real vector space
[23:19.52] okay
[23:22.88] we denote by
[23:26.08] the star the vector space
[23:33.20] of linear functions on the
[23:38.72] real valued linear functions
[23:44.16] linear functions f goes from
[23:47.28] v to r so this is the
[23:51.76] dual vector space associated to v now
[23:54.88] there's a little exercise for you
[23:56.96] please do these little exercises even as
[23:58.96] you're listening it doesn't take it's
[24:00.56] not so difficult
[24:01.52] uh but it's it's important for you to
[24:03.36] get practice at this kind of thing
[24:04.96] show v star is a real vector space
[24:10.64] this might require you to google
[24:14.16] the definition of a vector space and
[24:15.84] then to check that given some linear
[24:18.08] functions on a vector space that the
[24:20.32] linear combinations are also
[24:21.84] linear functions on a vector space
[24:25.60] so it's a nice little exercise for you
[24:27.20] to do to get warmed up in this
[24:28.88] uh what do we call elements of v star so
[24:31.92] elements
[24:32.88] of v star they have a name they're
[24:35.36] called dual vectors
[24:47.36] now given a basis for v we can furnish a
[24:50.56] basis for this dual vector spaces
[24:54.00] suppose someone comes along
[24:57.12] hands you a basis
[25:04.24] for v then you can furnish given this
[25:06.40] basis
[25:07.60] a basis for v-star
[25:22.16] so how do we define this basis
[25:25.52] well we'll denote the basis v
[25:28.56] with the the the index up
[25:31.84] this starts to be important now uh v mu
[25:35.12] star is the basis for our dual vector
[25:38.64] space and
[25:39.36] when you act with that basis on vnu you
[25:42.00] get
[25:42.48] delta mu nu for all mu
[25:45.84] nu so this is now the moment
[25:49.36] when it does matter if indices are up or
[25:52.32] down
[25:52.80] so this when you see indices up then
[25:56.00] you can think of these as elements of
[25:58.16] the dual vector space when you see
[25:59.44] indices down
[26:01.20] think of these as elements of the
[26:05.28] in this case the vector space v
[26:09.68] now here's some very important exercises
[26:12.32] that you should do
[26:13.04] right now the this is this will be used
[26:16.40] frequently and without without uh
[26:19.84] referring to this result if you do v
[26:22.96] star if you take the dual space of a
[26:24.96] vector space v and then you take the
[26:26.16] joule space of that vector space then
[26:27.84] you show that that's
[26:29.28] isomorphic or equivalent to the original
[26:30.88] vector space v
[26:34.64] you should really go ahead and do that
[26:37.36] exercise right now and pause the video
[26:39.28] because we're i'm going to make frequent
[26:41.76] use of that result without even
[26:43.52] referring to it anymore
[26:46.08] another consequence or result you should
[26:48.72] convince yourself of immediately is that
[26:50.72] the dimension of the vector space v star
[26:52.56] is v
[26:57.92] okay now we should say that
[27:03.12] this basis construction that i've just
[27:06.16] shown there
[27:06.72] gives a actual isomorphism between v and
[27:09.84] v star
[27:15.36] so if you've got an input basis v mu for
[27:18.08] v
[27:18.96] the procedure i've just described to
[27:20.48] construct a output basis v
[27:22.32] mu star gives
[27:26.96] an isomorphism i mean all vector spaces
[27:30.00] are the same dimension isomorphic right
[27:33.20] isomorphic is the same
[27:38.32] between v and v star
[27:42.08] but important note
[27:45.44] this is not canonical it depends on
[27:48.88] coordinates
[27:56.96] very important in physics if something
[27:58.88] is not canonical
[28:00.40] or depends on coordinates then it's not
[28:03.20] coordinate free it means that
[28:05.36] there is the identification depends on
[28:07.76] some human's choice of some basis
[28:09.76] vectors
[28:10.72] so had we chosen another set of basis
[28:12.72] vectors for v we would have gotten a
[28:14.08] different identification between b and b
[28:15.84] star
[28:16.64] you have to be aware that although these
[28:18.40] vector spaces are isomorphic and there
[28:20.32] is a way to present an iso isomorphism
[28:22.08] namely just construct these dual pairs
[28:23.92] of bases
[28:25.04] this is not the right one necessarily in
[28:28.16] physics
[28:29.60] because what you want in physics are
[28:31.28] statements that don't depend
[28:32.64] on coordinate systems chosen by humans
[28:35.04] you want
[28:36.00] statements that depend on physical
[28:39.84] uh on on procedures that can be defined
[28:43.60] with respect to an arbitrary coordinate
[28:45.28] system
[28:47.68] okay so that's an important uh
[28:50.80] new vector space you you'll have to
[28:54.00] spend a long time getting used to dual
[28:56.00] vector spaces because we're going to use
[28:57.36] them a lot in this course
[29:00.88] now i said we're not going to worry
[29:02.96] about direct sums in this course
[29:04.56] so now we're going to come to to the
[29:06.64] next important way of building a new
[29:08.56] vector space from an old vector space
[29:11.44] and that is that of the tensor product
[29:13.68] so i'm going to give you
[29:14.56] an informal definition
[29:18.40] then a slightly more formal definition
[29:20.40] and then third definitions this is a
[29:22.40] really important topic the tensor
[29:23.84] product
[29:24.56] and i'm going to dwell on this for the
[29:26.16] little while now for the following
[29:27.60] reason
[29:28.24] so throughout your studies you're going
[29:31.04] to meet the tensor product of vector
[29:32.48] spaces and of course
[29:33.84] tensor products play an incredibly
[29:35.36] important role also in many
[29:37.04] particle many body quantum mechanics
[29:39.44] there you need to take the 10
[29:40.88] we routinely take tensor products of
[29:42.48] vector spaces and of course
[29:44.48] quantum computers in quantum information
[29:46.48] theory is also concerned with a
[29:48.08] discussion
[29:49.28] concerned with many particles namely
[29:50.96] many qubits they will also
[29:52.96] need to make frequent use of the tensor
[29:54.72] product of vector spaces so i'll spend a
[29:56.40] little bit of time
[29:57.44] now giving the definition of the tensor
[29:59.68] product of two vector spaces so you can
[30:01.52] make reference to this in your future
[30:06.84] studies
[30:08.32] so the tensor product
[30:13.28] of two vector spaces
[30:23.84] v and w you can we're going to stick
[30:25.92] with finite dimensions throughout
[30:27.60] this course most likely
[30:30.64] you're i think we're mostly going to be
[30:32.40] considering finite dimensional vector
[30:33.92] spaces so i'll present this definition
[30:35.52] informally
[30:36.08] first for finite dimensional vector
[30:37.68] spaces so
[30:39.36] dim v is less than infinity and dim
[30:42.72] w is less than infinity so the tensor
[30:46.24] product of two vector spaces we have a
[30:47.76] notation for it is denoted
[30:51.76] the tensor w
[30:54.96] and what what is it well it consists of
[30:57.60] a whole bunch of vectors
[31:07.12] of the form e tensor f
[31:12.16] for for all e
[31:15.44] in v for all
[31:18.96] f in w and
[31:22.32] their linear combinations
[31:39.36] so that's what the tensor product
[31:41.68] consists of it consists of these funny
[31:43.52] elements written e tens of f
[31:47.12] and they're linear combinations
[31:50.40] so and this
[31:54.00] tensor product symbol obeys some special
[31:57.60] has some special properties
[32:05.68] i'll write them down i'm going to do
[32:07.04] this two more times actually
[32:08.88] to get you used to this so the elements
[32:11.36] which i've written
[32:13.44] uh e tensor if you know what i don't
[32:15.52] like f because f
[32:16.48] is the notation we've been using for
[32:19.12] scalar functions i'll use lowercase v
[32:22.16] tensor lowercase w
[32:27.04] so the elements uh v tensor f
[32:32.16] uh v tensor w uh obey
[32:35.76] the following bilinearity condition
[32:49.36] so if you have a linear combination of
[32:51.60] some v
[32:52.96] v1 and v2 and you take the tensor
[32:56.32] product so listen to the way i talk
[32:57.84] about these things that's the way people
[32:59.44] refer to these objects
[33:01.20] with another linear combination of
[33:03.68] vectors
[33:05.60] w1 and w2 then that's equal to
[33:10.72] a1 b1 v1 tensor w1
[33:14.48] plus a1 b2
[33:17.68] v1 tens of w2 plus
[33:21.44] a2 b1 v2 tensor w1
[33:25.68] plus a2 b2 v2 tensor w2
[33:30.32] for all a1 a2 in the real numbers for
[33:34.00] all
[33:34.48] b1 b2 in the real numbers
[33:39.28] and for all v1 v2 in the vector space v
[33:43.44] and for all w1 w2 in the vector space w
[33:53.44] there's the informal definition of the
[33:55.04] tensor product of two vector spaces
[33:56.96] v and w
[34:00.72] and that informal definition is most
[34:02.48] likely going to be fine for you
[34:04.40] in this course and in fact for the rest
[34:06.00] of your studies
[34:10.08] so take a moment to study that
[34:13.20] definition
[34:14.24] you'll have in the exercises no doubt
[34:17.92] a chance to look at some examples i
[34:20.40] highly recommend
[34:21.52] that you uh take a look at the following
[34:24.08] example
[34:25.68] as a exercise so you take i don't know
[34:30.88] let's take v is r2
[34:34.40] w is r2 okay
[34:38.48] form uh the vector space v
[34:41.52] tensor w which is r to
[34:45.04] tensor r two and
[34:48.08] uh take a look at some elements in this
[34:50.80] vector space
[34:51.92] just as an example
[34:56.00] look at i don't know this vector here
[34:58.00] one zero ten
[34:59.36] zero one
[35:03.52] and uh
[35:06.88] i know don't look at that in fact look
[35:09.36] at
[35:10.32] a b i'll even make it more consistent
[35:13.52] with what i wrote in the definition a1
[35:15.28] a2 tensor b1 b2
[35:19.12] so i want you to to look at that that
[35:22.00] that example element of v
[35:23.52] tensor w and r use the condition star
[35:28.84] above
[35:30.24] write that resolve that as a linear
[35:31.76] combination of four basis vectors
[35:45.28] one zero one zero etc
[35:49.12] and thereby convince yourself
[35:55.76] that r to tensor r2
[35:59.12] is isomorphic to r4
[36:03.44] that's a nice little exercise to get
[36:05.20] used to these
[36:06.80] this these uh constructions so now
[36:10.08] that's
[36:10.48] uh the informal definition of a tensor
[36:13.04] product that really will suffice
[36:14.72] for for the rest of this course however
[36:16.96] if you're interested in a little bit
[36:19.20] more complicated a little bit more
[36:22.32] formal and
[36:24.00] mathematically rigorous definition you
[36:26.32] could
[36:29.60] take a look at the following definition
[36:32.64] of the tensor product of two vector
[36:34.40] spaces
[36:37.68] so this is the hazardous terrain track
[36:39.52] you can totally skip this if you don't
[36:41.28] want to
[36:43.68] if you don't want to be bothered with
[36:45.76] formal rigorous mathematics
[36:47.76] so more precisely we can define the
[36:49.84] tensor product of two vector spaces v
[36:51.84] and w as
[36:56.08] the free vector space generated by the
[36:58.96] cartesian product of
[37:00.40] v and w so the cartesian product between
[37:02.24] vector spaces is a vector space should
[37:04.72] be
[37:05.04] uh is a set
[37:09.44] and so because the cartesian products of
[37:13.04] two vector spaces are set and not a
[37:14.56] vector space we have to turn it into a
[37:16.00] vector space so we need
[37:17.52] to use the so-called free vector space
[37:19.52] construction to do that
[37:23.04] where f denotes the free vector space
[37:34.84] generated
[37:37.12] by v cross w and what's a free vector
[37:39.20] space well given a set the free vector
[37:40.96] space over that set
[37:42.72] is just all finite linear combinations
[37:45.04] of elements of that set
[37:46.40] just definitely defined to be that set
[37:49.28] and
[37:50.00] but mod modded out by an equivalence
[37:52.00] relation
[37:54.72] so the free vector space over v cross w
[37:58.32] is too big as a thing right it's an
[38:00.16] infinite dimensional vector space
[38:01.52] because there's an
[38:02.16] infinite number of elements in v cross w
[38:04.56] and then all finite linear combinations
[38:06.32] of an infinite
[38:07.36] number of elements gives us
[38:09.44] automatically an infinite
[38:10.88] uh and all those elements are considered
[38:12.64] to be linearly independent
[38:13.92] gives us an infinite dimensional vector
[38:15.44] space so the free vector space is too
[38:16.96] big by far
[38:18.48] to capture this tensor product vector
[38:20.80] space that we have
[38:22.56] so what we do is we identify lots of
[38:24.32] elements in this massive infinite
[38:25.76] dimensional set
[38:27.12] via this equivalence relation
[38:33.52] twiddle
[38:40.16] and so this equivalence relation twiddle
[38:41.68] is defined by the following
[38:43.20] five properties so the first is
[38:46.72] identity
[38:50.56] one so v comma w
[38:53.84] is equivalent to w comma v sorry
[38:57.12] v comma w right that's what it means to
[38:59.20] be the identity
[39:00.24] so if you've got a v in big v a w and
[39:02.64] big w
[39:03.76] then under this equivalence relation
[39:05.36] they're equivalent that's what it needs
[39:06.80] to be for
[39:07.52] an equivalence relation we have
[39:09.04] something called the symmetry right you
[39:10.88] know
[39:11.12] so if v comma w is equivalent to
[39:14.32] v prime w prime then that implies that v
[39:17.84] prime w prime is equivalent to v comma w
[39:21.44] so we assume that of our equivalence
[39:23.12] relation that means this is a symmetry
[39:24.88] condition
[39:26.56] we also have transitivity
[39:34.72] activity which means that if
[39:38.40] v w is equivalent to v prime w prime and
[39:43.36] v prime w prime is equivalent to v
[39:46.16] double prime w double
[39:47.68] prime then that implies that v
[39:50.96] w is equivalent to v double prime
[39:55.12] w double prime it's transitivity
[39:58.32] okay so far the first three conditions
[40:00.00] here on this equivalence relation just
[40:01.36] say that it's an equivalence relation
[40:04.24] but now comes the interesting stuff that
[40:07.20] defines
[40:07.84] our equivalence relation distributivity
[40:14.40] uh this this is where this this
[40:16.80] condition has some bite
[40:18.00] and will lead to
[40:21.76] a non-trivial collapse on the elements
[40:23.92] in the free vector space generated by v
[40:25.44] cross
[40:26.48] w so
[40:29.52] this property we demand of the
[40:32.32] equivalence relation says that if we
[40:33.76] have the linear combination
[40:35.68] of v and v prime in the first slot then
[40:38.64] that's equivalent
[40:39.60] to adding these two elements v comma w
[40:43.04] and v
[40:43.84] comma v prime comma w separately
[40:48.24] and the same goes for the second slot so
[40:50.32] if you've got
[40:53.44] if instead you have w prime
[40:57.28] v comma w plus v comma w prime then
[40:59.84] that's equivalent to
[41:00.96] v w plus w prime okay
[41:04.24] and the last property we're going to
[41:06.00] demand of this equivalence relation is
[41:07.52] the scalar multiples
[41:10.80] are equivalent in the following
[41:14.40] sense namely c some scalar number times
[41:18.48] v comma w
[41:20.40] is equivalent to c times v w
[41:25.60] which is equivalent to v comma cw
[41:28.80] for all c in r and of course
[41:32.24] these are defined by for all u in i'm
[41:35.12] sorry
[41:35.52] v in v
[41:38.56] and for all little w in big
[41:41.76] w so we have this equivalence relation
[41:45.60] defined by these five properties
[41:47.44] and then the tensor product of v and w
[41:50.24] is just
[41:50.72] the free vector space that we mod out by
[41:54.48] this equivalence relation so if you're
[41:56.24] familiar with this kind of
[41:58.32] construction the equivalence relations
[42:00.40] on sets then this could
[42:02.40] potentially make you a lot more
[42:04.40] comfortable with
[42:05.52] the tensor product of two vector spaces
[42:12.00] now we've got a name for certain
[42:13.44] elements in the vector spaces
[42:15.44] so this is the end by the way of the
[42:19.60] advanced material we have a name for
[42:22.96] elements of
[42:23.84] v tensor w
[42:28.72] or some special elements of v tensor w
[42:37.12] when they have a certain structure so we
[42:38.96] have
[42:40.56] their definition either using the
[42:43.04] advanced material
[42:44.00] the the informal definition the elements
[42:45.76] of v tensor w are linear combinations
[42:47.84] right there's
[42:49.44] they're not always of the form
[42:54.88] v tends to w but they're linear
[42:56.64] combinations of so-called simple tenses
[42:59.76] so a simple tensor is one that has the
[43:01.84] form
[43:03.12] v tensor w
[43:08.48] for vj
[43:13.36] so any tensor that has the form any
[43:16.40] element that has the form of a v
[43:18.24] tensor w is called a simple tensor and
[43:22.16] and we have that all elements of v
[43:25.28] tensor w are per definition linear
[43:27.04] combinations of simple tenses so if you
[43:29.04] have
[43:30.48] an arbitrary
[43:35.68] u element in v tends to w
[43:40.80] it's a big mistake to say that u always
[43:43.92] has the form of v
[43:44.96] tends to w instead
[43:49.12] the only thing we can say for sure is
[43:52.84] that
[43:55.92] u has the form of a linear combination
[43:59.04] of such vectors
[44:02.56] such simple tenses
[44:06.64] so here big m is the dimension of v
[44:11.68] big n is the dimension of w
[44:17.12] so it's a big mistake to think that uh
[44:20.24] okay a warning because i see this happen
[44:22.88] so often
[44:24.72] warning
[44:30.88] uh for all
[44:37.44] given you in a tensor product of vector
[44:40.24] spaces
[44:42.72] it is not
[44:45.76] always true
[44:50.08] that u is of the form of v tensor w
[44:54.88] the only thing we can say for sure is
[44:56.56] that it's a linear combination
[45:00.48] of such things
[45:05.68] so it's not not always true
[45:08.88] that blah so generically
[45:17.60] u is not always of the simple tensor
[45:21.20] form
[45:21.60] simple tenses are very special in tensor
[45:23.52] product vector spaces
[45:28.00] okay so we're almost done with the first
[45:30.16] approach
[45:31.28] the first two approaches to defining uh
[45:33.52] tensor products of vector spaces
[45:35.12] uh there's a lot you can do here you can
[45:36.72] spend a long time building up the
[45:38.40] the rigorous mathematics of tensor
[45:39.92] products of vector spaces we don't
[45:41.12] really need any of that we just need to
[45:43.57] [Music]
[45:45.12] make reference to these already simple
[45:47.44] facts and as long as you can make the
[45:48.80] manipulations
[45:49.68] according to these rules then you'll be
[45:52.32] fine
[45:52.88] uh generically that so there is one one
[45:55.68] lemma
[45:56.32] which is an exercise i guess for this
[45:58.32] course um
[45:59.76] the dimension of the vector space v
[46:02.16] tends to w the one you get by tensoring
[46:04.00] being w
[46:05.36] is because we're working with finite
[46:07.44] dimensions it's just the product of
[46:08.96] these dimensions here you can show that
[46:11.12] that's an exercise for you and
[46:14.48] also it's worth saying that if you have
[46:18.00] three vector spaces then taking the
[46:19.92] tensor products
[46:21.12] it doesn't matter which order you take
[46:22.40] the tensor products in you can also show
[46:25.12] that uh
[46:28.16] these two vector spaces are equivalent
[46:30.48] so we just use the notation
[46:35.84] u tensor v tends to w without worrying
[46:38.48] about the order in which we did the
[46:40.84] tensoring
[46:42.08] okay so that's i think a brief summary
[46:45.20] of the notions
[46:48.32] of tensor product of the notion of a
[46:51.52] tensor product of two vector spaces
[46:53.68] now i'm going to introduce to you
[46:55.04] another approach to defining tensor
[46:57.12] products of vector spaces
[46:59.04] and it's a little bit uh given i i
[47:01.92] consider this first
[47:02.88] approach the the more intuitive notions
[47:04.96] certainly the one that appears to
[47:06.40] advance quantum mechanics and quantum
[47:07.84] information theory many body theory and
[47:09.36] so on and so
[47:10.00] forth but the the way that people
[47:12.24] present the tensor product in general
[47:13.60] relativity is is
[47:15.36] a little different it's uh the dualized
[47:18.80] version of this definition
[47:20.48] essentially so that's the approach that
[47:24.72] we're going to take a look at now
[47:25.92] approach two to
[47:29.20] second approach to tensor products this
[47:31.04] is what you what we might call
[47:33.04] the dual version
[47:36.16] of the tensor product
[47:40.24] this is something that you'll see in the
[47:42.72] general relativity textbooks including
[47:44.80] world's textbook so
[47:51.20] it starts off the same so let v and
[47:54.72] w be real vector spaces
[47:57.84] uh i probably will forget to see real
[48:00.80] vector space for the rest of this course
[48:02.40] whenever i say vector space in this
[48:03.68] course i mean real unless i tell you
[48:05.20] it's a complex vector space
[48:08.72] so according to this definition we talk
[48:11.36] of things called tensors now
[48:12.80] a tensor which yeah okay there is some
[48:15.04] chance for confusion with the notion of
[48:16.64] the
[48:17.92] simple tenses i introduced in the
[48:19.28] previous definition
[48:20.96] i hope that you will will be able to
[48:24.16] cope with the slight overloading of
[48:25.84] notation
[48:27.12] but you're stuck with it that's how they
[48:29.12] refer to it in the textbook so we're
[48:30.64] going to have to follow the textbooks
[48:31.92] what is a tensor here
[48:33.04] a tensor is a multi-linear map
[48:42.24] t which goes from the cartesian product
[48:45.44] of these two
[48:46.16] vector spaces to the real numbers and
[48:49.44] this multi-linear map is multi-linear
[48:51.52] what does that mean it has to obey some
[48:54.16] uh conditions which mirror all these
[48:57.36] bi-linearity conditions we imposed in
[48:59.44] the via this equivalence relation in the
[49:01.12] previous definition
[49:02.80] so where if you apply t to
[49:09.04] the cartesian product of linear
[49:10.40] combinations then it distributes
[49:12.32] in the right way over addition
[49:39.84] for all a1 a2
[49:43.36] b 2 in the real numbers and v 1 v
[49:46.56] 2 in the vector space v
[49:50.08] and w 1
[49:53.92] w 2 in the vector space w
[49:56.96] so this is the way uh the tenses tenses
[50:00.08] intensive products of vector spaces are
[50:01.52] presented in many general relativity
[50:03.76] textbooks
[50:05.04] um that's gonna be the case for this
[50:08.24] course
[50:08.96] we're gonna have to also make reference
[50:11.76] uh
[50:12.08] to this definition here so why is it
[50:15.20] that this
[50:15.68] apparently completely different way of
[50:17.28] defining tensor products of vector
[50:18.64] spaces is
[50:19.68] is equivalent well because
[50:22.72] uh this this notion here is just the
[50:25.04] jewel of the notion i've already
[50:26.16] presented
[50:28.24] so this is the we'll call this the
[50:30.48] classical
[50:33.44] approach to tensor
[50:36.64] products and
[50:40.56] it's just the jewel of the original
[50:42.32] definition
[50:45.04] so the space of all these multilinear
[50:47.36] maps is just the jewel
[50:53.52] all right and i've told you about dual
[50:54.80] vectors pieces
[50:57.36] so what we'll call the formal the
[51:00.40] informal definition
[51:03.60] the formal informal definition before
[51:07.12] is different from the so-called
[51:08.24] classical approach
[51:12.08] but how is it different well if we take
[51:14.80] v tensor w as defined
[51:16.48] previously and take the joule of that
[51:19.84] vector space then what do we get
[51:21.36] well we get the set of all t that maps
[51:24.48] from v
[51:24.96] to w to r
[51:28.00] such that t is multilinear
[51:32.64] so actually you can think of the
[51:33.76] classical tenses that we see in
[51:35.20] differential geometries just elements of
[51:36.72] the joule of v tensor w
[51:41.20] and that's the way to
[51:46.88] unify these two approaches to tensor
[51:48.64] products it's very important
[51:50.80] that you convince yourself that v
[51:53.84] tensor w joule is the same as
[51:57.44] v dual tensor w joule
[52:03.28] otherwise nothing nothing
[52:07.12] further what i may talk about in the
[52:09.12] rest of this lecture will make sense to
[52:11.04] you
[52:17.52] okay so now i've covered two approaches
[52:21.12] to defining tensor products
[52:22.88] of vector spaces
[52:26.08] and the rest of this lecture is going to
[52:28.48] be concerned with
[52:29.68] applying these constructions to the
[52:32.72] vector space that we have in
[52:33.68] differential geometry namely vp
[52:35.68] the tangent space
[52:50.88] so that's our atom right our atomic
[52:54.48] object is vp the tangent space uh the
[52:57.28] point p
[52:57.92] of a manifold and
[53:01.28] now we have some things we can do so we
[53:02.88] can introduce
[53:05.76] the joule of that vector space so we
[53:07.68] call that vp star
[53:09.92] this is the first vector space we can
[53:12.16] build now that we've got
[53:13.44] these new operations and we can build
[53:17.36] uh this vector space right v tensor v
[53:20.16] tensor
[53:22.08] v tensor v star tensor v
[53:25.60] star
[53:35.84] we can also build um
[53:43.36] slightly more convoluted vector spaces
[53:45.92] we could do
[53:46.56] v tensor v star or sorry i've got to put
[53:49.28] subscripts p
[53:50.16] everywhere right now we're doing it on
[53:52.24] the tangent space
[53:54.72] and we could also build this vector
[53:57.12] space
[53:58.72] v p tensor vp star tends to vp
[54:02.16] right um it turns out they just don't
[54:05.60] play
[54:06.24] as big a role uh in a lot of the
[54:09.12] differential geometry
[54:10.00] we're doing we're really gonna it's
[54:11.84] really sufficient to look at the vector
[54:13.60] spaces of this
[54:14.80] former type where we cluster all the b's
[54:16.80] together and all the vp stars together
[54:18.88] it just is and um for
[54:24.88] it just these other vector spaces just
[54:27.04] don't play as prominent a role
[54:28.80] in differential geometry because um
[54:39.84] the order of the arguments of of tenses
[54:42.40] just isn't that important
[54:52.72] in quantum information theory on the
[54:54.24] other hand such that the order of tensor
[54:56.56] product factors plays a very important
[54:58.32] role
[54:59.92] less so in the theory of of identical
[55:02.40] particles but still
[55:03.52] if you have different types of identical
[55:04.88] particles the order of the tensor
[55:06.08] product factors will again matter
[55:08.32] in but in intensive product in general
[55:11.52] relativity we won't be focusing on
[55:14.16] vector spaces where the uh tensor
[55:17.12] product factors
[55:18.00] are mixed up in this funny
[55:19.79] [Music]
[55:20.92] non-contiguous way
[55:22.32] we're really only going to focus on the
[55:24.64] tensor products or vector spaces of this
[55:26.40] type
[55:27.68] here
[55:31.04] now elements of this circled type here
[55:34.96] can also be interpreted as
[55:38.32] elements as multilinear maps
[55:45.92] so elements of i don't know what we'll
[55:48.48] call this
[55:49.76] equation here uh oh yeah i'll give this
[55:52.88] a notation i'll give this thing a
[55:54.08] notation right now because it's one that
[55:55.68] we're going to need
[55:57.12] a lot well i'll i like this notation vp
[56:00.96] superscript tensor k tensor v
[56:04.48] p star superscript tensor l
[56:08.08] so that hereby defines this notation
[56:12.72] we're gonna use this notation somewhat
[56:16.24] frequently
[56:17.84] in this course
[56:29.52] and elements of vp tensor k
[56:32.80] tensor vp star
[56:36.40] tensor l we can interpret these elements
[56:40.56] also as multilinear maps
[56:58.56] uh they can be interpreted as
[57:00.40] multi-linear maps that go
[57:02.24] from vp star tensor k
[57:08.32] tensor vp tensor l
[57:14.96] and why is that
[57:20.00] well uh the double dual
[57:23.04] of a vector space is the same as the
[57:24.72] original vector space right you
[57:26.08] already know that that's that's one of
[57:28.40] these exercises
[57:29.52] that we've had before so vp
[57:33.36] star star is equivalent to vp original
[57:38.16] and therefore
[57:41.60] an element of so
[57:44.64] let's take some element t
[57:48.40] of v p tensor k
[57:52.40] tensor v p star tensor l
[57:55.92] an element of that is equivalent to an
[57:58.16] element of
[57:58.96] v p star
[58:02.96] let me get this right
[58:15.44] an element of vp let's
[58:18.88] start it that way
[58:22.32] so if you've got some element of vp you
[58:24.48] can think of it
[58:25.36] as
[58:34.80] let's not do too many steps at once
[58:36.88] thought of as
[58:38.48] a function
[58:41.76] on vp star
[58:45.84] right because vp star star is equivalent
[58:48.72] to vp
[58:49.44] so an element of vp can be thought of as
[58:51.20] a function on vp star
[58:52.48] as well
[58:56.96] and therefore an element of v p
[59:00.56] tensor k
[59:12.56] tensor vp star tensor l
[59:19.04] can be thought of
[59:22.64] as an element of vp
[59:25.84] star tensor k
[59:30.16] tensor bp tensor l joule
[59:54.32] so you can think of one of these
[59:58.40] elements of this this vector space here
[60:00.56] really as an element of the double joule
[60:04.48] or as a function a multi-linear function
[60:12.84] right
[60:16.64] of just this stuff inside the brackets
[60:21.84] good this turns out to be very useful
[60:26.08] and we will use this double dual
[60:28.32] isomorphism loads in this course
[60:30.08] you get used to it so such
[60:33.84] a tensor t
[60:39.04] is uh such a map
[60:46.32] is a tensor of
[60:49.84] type kl
[60:56.64] there's a the vector space of all such
[60:58.48] linear maps has a name
[61:11.52] so the vector space of all tensors of
[61:13.28] type
[61:18.00] kl is denoted
[61:22.80] calligraphic t k comma l
[61:26.48] and we have that the dimension just from
[61:29.12] the the
[61:30.24] exercises above the dimension of
[61:33.92] t calligraphic t k comma l is
[61:37.12] n to the k plus l where the dimension
[61:41.28] of our tangent space is
[61:44.48] little n
[61:47.84] let's consider now some example
[61:51.84] low type tenses
[62:00.40] so our first example is the case where k
[62:03.92] equals zero l equals one so a tensor of
[62:07.04] type
[62:09.92] zero comma one is an
[62:18.84] element
[62:22.08] of the vector space v
[62:25.20] tends to zero v p tends to zero tensor v
[62:28.72] star tensor one now we've got here a
[62:32.32] notational problem right i've
[62:34.00] now got the to take the zeroth tensor
[62:37.44] product of a vector space
[62:39.60] uh and we just define
[62:43.68] the zeroth tensor product of a vector
[62:45.68] space just to be the complex numbers
[62:47.52] themselves
[62:48.40] that not the complex numbers the real
[62:50.16] numbers
[62:55.12] so this is just the real numbers tensor
[62:58.16] v p star
[63:02.16] uh and the real numbers tends to
[63:04.08] anything any vector space is just the
[63:06.08] vector space itself
[63:07.36] so this is just an element of vp star so
[63:10.40] a tensor of type
[63:11.68] zero comma one is a dual vector
[63:26.24] a tensor of type
[63:30.96] one comma zero is an element
[63:37.76] of
[63:41.44] v p tensor v p star
[63:45.44] tends to zero
[63:48.88] which is just v p tends to the real
[63:50.80] numbers which is just v
[63:52.40] p so it's just a vector
[63:59.84] and let's just go back and look at our
[64:01.04] other notations that we've been talking
[64:03.04] about here
[64:04.32] so a tensor of type kl is a multi-linear
[64:08.08] map from vp star tensor k to
[64:10.32] vp tensor l to the real numbers
[64:15.28] but you can also think of it simply as
[64:17.12] an element of
[64:18.72] the vector space v p tensor
[64:22.24] k v p star tensor l
[64:27.44] and that that notation is coherent with
[64:29.36] these definitions that i've just given
[64:30.88] there
[64:31.36] and these two examples uh just
[64:34.16] illustrate
[64:35.84] that uh tenses of low type elements of
[64:38.96] the vector space vp
[64:40.08] and its joule accordingly now the first
[64:43.28] interesting tensor that we can construct
[64:46.40] so in fact we haven't really constructed
[64:48.00] a new vector space so far with these two
[64:49.84] examples
[64:51.12] the first interesting new vector space
[64:54.08] that we have constructed
[64:55.52] in this fashion is that of an element of
[64:58.08] type 1 1.
[65:11.52] is an element
[65:16.64] is it is a well let's say it's an
[65:18.88] element yeah
[65:22.32] of vp tensor vp star
[65:25.92] or equivalently
[65:32.24] multi-linear map
[65:39.04] from vp star
[65:43.68] tens of vp to the real numbers
[65:48.40] and we're going to identify this vector
[65:49.68] space with the space of linear
[65:51.04] transformations
[65:52.08] on vp or matrices
[66:05.20] so we can actually think of elements of
[66:06.56] this tensorboard vector space in a
[66:08.08] variety of ways
[66:09.20] they're vectors in a bigger vector space
[66:12.24] they are also can be interpreted as
[66:15.60] uh as joule vectors in another vector
[66:18.56] space or they can be equivalently
[66:19.76] interpreted as linear transformations
[66:32.64] so we can interpret t element of
[66:36.72] uh of type let's see do it this way
[66:42.40] of type one one in three ways
[66:48.48] and as we get to bigger types you get
[66:50.08] more and more interpretations that are
[66:51.44] available to you
[66:53.44] so one well you know as per definition
[66:56.24] it's a vector
[66:57.84] in the vector space
[67:04.00] vp tensor vp star you can think of it
[67:07.04] that way if you like
[67:08.40] as a multi-linear map
[67:16.08] from vp star tensor vp to the real
[67:20.16] numbers
[67:21.52] you certainly need to give it that way
[67:22.72] if you wish as a dual vector
[67:39.36] or you can think of it in a third way
[67:42.32] that a new possibility arises now and
[67:44.40] this will bring us to the topic of
[67:45.76] tensor contractions
[67:47.44] as a linear transformation
[67:51.44] or matrix or linear map
[68:02.96] and this this linear transformation goes
[68:04.96] from
[68:05.27] [Music]
[68:07.12] you can think of it as a linear
[68:08.16] transformation t
[68:10.48] now put two underlines in that now to
[68:12.24] indicate that i'm thinking of it as a
[68:13.52] matrix
[68:14.32] that goes from v p to v p
[68:17.36] so let's uh let's uh convince ourselves
[68:20.24] that
[68:20.72] this is possible
[68:26.24] so i'll show you how now so the way this
[68:28.40] works is
[68:30.08] i'll just do it with respect to a basis
[68:31.84] so choose
[68:33.36] a basis but the result is basis
[68:36.84] independent
[68:38.40] the interpretation of t is a linear
[68:39.84] transformation is bases independent
[68:41.68] so choose a basis
[68:43.45] [Music]
[68:46.32] v mu of the vector space v
[68:50.84] p now the first thing to note is that
[68:54.24] t is an element of this vector space v p
[68:58.32] tends to v p star so it it may itself be
[69:00.80] written as a linear combination
[69:06.24] of these basis vectors remember that
[69:09.44] given a basis of vp we get automatically
[69:12.48] a basis
[69:16.24] of vp star it's not canonical but
[69:20.48] it's not the only basis you can choose
[69:22.80] but you can choose that as a basis
[69:24.80] and so these numbers here are elements
[69:27.36] of the real numbers
[69:28.80] t mu nu now notice i'm going to start
[69:31.20] being a bit more consistent about
[69:33.28] the placement of superscripts and
[69:36.24] subscripts
[69:37.04] and they tell you which elements
[69:38.80] correspond to
[69:41.20] parts in the vector space vpn which
[69:43.36] correspond to elements
[69:45.04] in matrix elements in the vector space
[69:48.08] vp star
[69:51.60] so once you've chosen a basis v mu of vp
[69:54.40] you get one for free
[70:00.84] for
[70:02.88] uh the vector space vp
[70:05.92] star and therefore you get this basis
[70:09.84] uh for v tensor v p tends to vp star
[70:16.16] and so we get a basis representation for
[70:19.76] t
[70:21.60] in terms of these simple tenses here
[70:27.12] and this is a simple tensor basis of
[70:30.16] vp tensor vp star
[70:35.28] and uh we call
[70:43.36] these coefficients lowercase t
[70:45.36] superscript mu
[70:46.40] lower uh subscript nu the matrix
[70:48.96] elements
[70:54.40] of t and this this this definition is
[70:57.44] justified
[70:58.40] because we can induce a matrix product
[71:02.08] structure on these vector spaces as
[71:03.68] follows
[71:07.36] i mean t mu nu is a matrix right
[71:11.36] the matrix t acts
[71:15.20] on elements
[71:22.24] of vp as follows so you can take it to
[71:24.88] act this is a definition
[71:26.80] you you can just ignore the matrix
[71:28.64] structure if you want to don't have it
[71:29.92] act on elements of vp but let's take it
[71:32.00] act
[71:32.80] so let um
[71:40.96] and let me get rid of this
[71:44.40] let w be an element of vp
[71:47.76] now i'm going to define the action of
[71:50.24] this matrix t
[71:51.36] on w as follows so t applied to w
[71:54.88] is defined to be this
[71:59.52] just write everything in components
[72:09.20] now juxtapose writing w
[72:13.12] over here so w is a vector now if you
[72:15.44] look at this
[72:16.48] there's a natural place for w to go and
[72:18.64] that is w can be acted on
[72:20.56] by these dual elements of the dual
[72:22.24] vector space so v mu
[72:24.40] star naturally can act be taken to act
[72:27.92] on w so we do exactly that so that's the
[72:29.68] definition here in the next line
[72:41.84] so now in the next line we take
[72:45.84] v new star to act on w it'll give us a
[72:49.12] number
[72:51.76] what number will it give us well we've
[72:53.60] got to write out w now in terms of this
[72:55.36] basis to find out what number that is
[73:16.80] and then we've got v new star and per
[73:19.84] definition that's the chronic delta when
[73:21.60] applied to
[73:22.24] v lambda
[73:28.40] and we got this number w lambda coming
[73:30.40] along for the ride
[73:48.56] and that's the result so we actually
[73:50.32] have found
[73:52.32] that
[73:59.12] the mute
[74:03.12] coefficient of t applied to w is none
[74:06.56] other
[74:07.04] than the sum oops
[74:11.28] over nu t mu nu
[74:14.32] w nu in other words it's just as though
[74:16.80] we've multiplied t by w so this
[74:28.24] procedure whereby we take two tenses
[74:30.72] and we put a vector into the dual slots
[74:34.00] of another
[74:34.96] is an example of something called tensor
[74:37.04] contraction
[74:41.04] so this procedure produces an element
[74:44.40] of vp or vp tensor products of vp
[74:47.44] vp star in a basis independent fashion
[74:52.64] so even though we took this action here
[74:56.16] it looks like we did we did it with
[74:57.44] respect to a basis right it looks like
[74:59.04] it looks completely bases dependent
[75:01.12] you can argue that you would have gotten
[75:02.96] the same result no matter what basis we
[75:04.48] chose
[75:09.28] so that's the final topic uh for
[75:13.28] pretty much the final topic for today's
[75:14.80] lecture namely contraction
[75:19.28] as a procedure to build tenses from
[75:22.16] other tenses
[75:29.52] so contraction is is a procedure whereby
[75:33.36] you take
[75:33.92] a complicated tensor
[75:38.00] and you can tenses in larger of a larger
[75:41.76] type or bigger type or more complicated
[75:43.68] tenses and build smaller tenses
[75:47.44] so it's a map c from
[75:50.96] the vector space
[75:56.08] t k comma l tensors of type k and
[75:59.36] comma k l and given a tensor of that
[76:02.88] type we we furnish or produce
[76:04.96] a tensor of type k minus 1 l minus 1
[76:14.16] and i'll just define it with respect to
[76:18.48] an arbitrary tensor in tkl
[76:23.76] how does contraction work
[76:26.88] so i have to tell you
[76:33.52] which slots are getting
[76:36.96] uh contracted so actually contraction
[76:40.24] isn't a unique map from tkl to tk minus
[76:43.44] one l minus one it actually depends on
[76:45.36] what
[76:45.92] tensor sub factor you're going to apply
[76:48.08] to
[76:49.60] and i'll now define this notation here
[76:53.44] so given a tensor t of type kl
[76:57.04] the contraction of the jth
[77:01.52] of the j slot and the j prime slot of t
[77:05.04] is defined as follows
[77:09.84] it's the tensor you get by
[77:13.20] you now interpret the tensor as a
[77:14.72] multilinear map by the way in order to
[77:16.72] make this definition
[77:18.48] in order for this definition to make
[77:19.84] sense
[77:23.60] it's the tense you get by putting in the
[77:26.40] jth
[77:28.84] argument some bases
[77:33.76] and in the j prime
[77:36.88] argument
[77:40.00] the joule basis or the on primal basis
[77:47.76] where the sigma
[77:50.96] is a basis
[77:55.36] for the vector space vp
[77:58.72] v sigma star is the corresponding joule
[78:02.08] basis
[78:08.08] and we interpret
[78:13.20] t as a multilinear map
[78:21.44] from v p star
[78:26.72] tensor k tensor v p tends to l
[78:30.56] to the real numbers
[78:35.20] and so then you can see that t when you
[78:38.08] interpret it as a multilinear map has k
[78:41.28] dual vector arguments followed by
[78:44.48] l tangent vector arguments and that's
[78:47.76] why it makes sense to stick in
[78:49.60] those particular basis elements in those
[78:51.28] positions in that multilinear map
[78:57.68] so if you write
[79:01.84] this element t in the definition
[79:07.04] in terms of a basis
[79:13.92] then you can see you can always write
[79:19.92] t in the following fashion
[79:26.72] you know once we've selected this basis
[79:28.32] v sigma then then we get automatically a
[79:30.80] basis
[79:31.52] for uh
[79:35.76] automatically a basis for any tensor
[79:38.00] product of the vector spaces
[79:39.84] and so we can always write any tensor as
[79:42.16] a linear combination
[79:45.20] of these basis vectors
[80:04.96] so once you've you've got an arbitrary
[80:07.84] tensor you can write it in terms of this
[80:09.84] simple tensor basis that you
[80:11.52] that's induced by your your basis of the
[80:14.40] tension space vp and then once you've
[80:18.80] you've expressed your tension in that
[80:20.24] that that
[80:21.20] that basis you can
[80:24.32] do the following exercise to understand
[80:26.24] what does the contraction of that tensor
[80:28.24] look like
[80:29.20] or what are the the the matrix elements
[80:31.44] of the contraction
[80:33.52] look like and you can
[80:37.44] do the following exercise you can show
[80:39.28] that in fact the matrix elements of the
[80:41.20] contracted tensor have to have the
[80:42.64] following form
[80:59.60] okay if you go ahead and
[81:02.80] substitute in the definition of the
[81:05.28] contraction
[81:07.28] into this uh basis representation you
[81:10.72] should find that the coefficients of the
[81:12.24] contractor tends to have the form that
[81:13.68] i've indicated here
[81:16.08] that's one of quite a few operations you
[81:18.08] can do on tensors to get new tensors
[81:19.84] from old ones
[81:20.96] so there's continuing this theme of
[81:24.24] building new from old there's another
[81:26.64] operation that we actually
[81:28.24] need to introduce before we can really
[81:30.08] make sense of this matrix product that i
[81:31.92] defined earlier as a
[81:34.56] uh as a contraction namely that of an
[81:36.96] outer product
[81:38.56] so if you've got a tensor of type
[81:42.40] kl and another tensor
[81:45.92] of some other type k prime l prime
[81:51.04] then you can build a bigger tensor
[81:57.36] called the outer product
[82:03.12] which is confusingly using this tensor
[82:06.00] notation
[82:07.04] sorry about that that's just the way
[82:08.64] notation we're stuck with
[82:15.60] so what is it it's simply the tensor u
[82:20.16] it's a new tensor u inside a much bigger
[82:23.68] or bigger
[82:24.16] type namely l or k plus k prime
[82:27.20] l plus l prime
[82:31.28] with components
[82:35.36] so if we write out u in terms of a basis
[82:38.00] for
[82:38.64] for
[82:41.68] t k plus k prime l plus l prime then
[82:44.72] you'll find that the components or we
[82:46.24] can define the components of u
[82:48.64] to be of the following form
[82:57.92] namely just get all the components for s
[83:01.12] and concatenate them with the components
[83:04.84] for tea
[83:24.40] i.e u or s
[83:27.76] tensor t
[83:32.32] is an element of
[83:36.64] a bigger tensor product space so you can
[83:38.72] just take the you have to reorder
[83:40.48] it it's an element of v tensor blah blah
[83:43.04] blah
[83:44.08] v p
[83:47.28] uh but now k plus k prime times
[83:57.84] and uh tensored with v p star l plus l
[84:01.36] prime times
[84:02.56] so you have to
[84:07.20] is uh
[84:11.84] you you know to form s tensor t you have
[84:14.96] to
[84:15.44] stick these vector spaces together but
[84:16.96] then reorder them because
[84:30.08] so you've got to reorder these tensor
[84:31.44] factors so that all uh
[84:33.60] vp factors lie on the left
[84:41.84] so for the most part as i said
[84:43.76] throughout general relativity the order
[84:45.44] of the vps and the vp stars are always
[84:47.28] taken so that the vps come first and the
[84:49.04] vp starts come second and there's no
[84:51.04] no particular problems in general
[84:52.32] relativity doing that later on
[84:54.80] in your studies perhaps when you do many
[84:57.20] particle quantum mechanics that will
[84:58.64] turn out to be
[84:59.68] more subtle and you'll have to worry
[85:00.88] about orders but we're not going to need
[85:02.56] to worry about ordering because we have
[85:04.16] lots of symmetric
[85:05.20] tenses in in differential geometry and
[85:07.28] general relativity
[85:09.68] okay that's it for today's lecture
[85:12.72] thank you very much your attention and
[85:14.32] see you next time
