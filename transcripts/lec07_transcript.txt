# General Relativity, Lecture 7: abstract index notation, curvature, parallel transport
# YouTube: https://www.youtube.com/watch?v=Mr9MND4OvYs
# Auto-generated transcript

[00:00.08] hello and welcome back to introduction
[00:01.68] to general relativity
[00:03.36] in the previous lecture we ended with
[00:05.36] something that was called
[00:07.04] abstract index notation and that with
[00:09.44] that topic i'd like to commence today's
[00:11.36] lecture with a little review and
[00:13.52] expanding a bit on the
[00:15.76] the notion of abstract index notation
[00:19.12] i believe that abstract unit index
[00:21.20] notation is a very helpful and useful
[00:22.88] tool in discussing tenses in general
[00:24.80] relativity and in fact tensors in
[00:26.56] general however i can anticipate that
[00:29.36] some of you will find this material a
[00:30.88] little confusing so i will spend some
[00:32.96] additional time
[00:34.56] at the beginning of today's lecture
[00:36.32] explaining abstract index notation in a
[00:39.04] little bit more detail
[00:43.04] if you recall
[00:46.24] from the previous lectures
[00:49.68] there's two ways to think about a tensor
[00:51.84] t that lives
[00:54.48] inside the space
[00:56.00] of tenses of type k comma l
[00:59.52] so the first way to think about a tensor
[01:02.72] of type
[01:03.92] k comma l
[01:05.76] as
[01:07.44] is as an element
[01:09.12] of a vector space in this case we're
[01:11.12] specializing now for the rest of this
[01:12.56] course to the
[01:13.84] tangent space to a manifold at a point p
[01:19.84] you can think of a tensor type kl
[01:22.16] as an element of a tensor product vector
[01:24.96] space so you just take these vector
[01:26.64] spaces
[01:28.24] form the tensor product
[01:30.08] k times and then l times and you can
[01:32.40] think of potential type k comma l as an
[01:34.64] element of this tensor product vector
[01:36.16] space so a tensor is an element of a
[01:37.92] vector space right that's the first way
[01:44.48] so i should say there's at least two
[01:46.56] ways to think about a tensor type kl
[01:50.64] because of course it turns out
[01:52.96] that there's many others
[01:54.72] and we use these other ways of thinking
[01:56.24] about tenses frequently but we dwell on
[01:58.40] that at the moment
[01:59.84] so
[02:00.72] at least two ways to think about
[02:03.36] t t as an is it as an element of a
[02:06.00] vector space so it's just a vector right
[02:07.92] so that's something that's important
[02:16.48] something important to highlight that a
[02:18.16] tensor can be thought of just as a
[02:19.76] vector
[02:23.36] and
[02:25.28] also
[02:26.32] there's another way to think about kind
[02:27.60] of the extreme opposite way of thinking
[02:29.36] about a tensor type k
[02:33.60] plus a k comma l you can think of a
[02:35.44] tensor
[02:36.64] as a map a linear map
[02:40.40] from a vector space
[02:45.68] to the real numbers and and what vector
[02:47.76] space well
[02:50.08] you have to dualize everything right
[02:51.52] remember the double jewel of a vector
[02:53.36] space is isomorphically the vector space
[02:55.36] itself
[02:57.28] so if you choose to think about your
[02:58.96] tensor as a linear map from a vector
[03:00.96] space
[03:05.60] so a linear map
[03:08.16] you can think of tensor as a linear map
[03:10.72] from
[03:11.76] a vector space
[03:15.28] to r okay that's another way of thinking
[03:17.60] about a tensor
[03:22.08] and the second way
[03:24.08] is often the way that people will work
[03:26.64] with tensors and general relativity of
[03:28.64] course it turns out there's a whole
[03:29.76] bunch of intermediate ways of thinking
[03:31.52] about a tensor i'll say it in words now
[03:34.16] we will see it in action later you can
[03:36.72] think of a tensor as a map from a vector
[03:39.12] space to a vector space and which vector
[03:40.88] space well you're free
[03:43.44] you could sort of uh you've got k plus l
[03:45.92] arguments here you could say i want uh
[03:48.00] to think of this tensor as a map from
[03:50.16] the first k arguments
[03:52.08] to so the vector space uh
[03:54.64] given by the last l arguments but you
[03:56.32] don't have to stick with k l you could
[03:57.60] say i could think i want to think of the
[03:58.88] tensor as the map from the first
[04:00.00] argument to the last k plus l minus one
[04:05.60] arguments anyway that's that's
[04:08.80] jumping ahead a little bit
[04:10.88] before we do that i want to think uh
[04:12.96] talk about ways to specify tenses or at
[04:15.12] least talk about tenses
[04:19.60] so in
[04:20.88] and there's this notation that i
[04:22.40] introduced in the previous lecture makes
[04:23.84] it uh is
[04:25.84] is a very simple notation that will save
[04:28.08] us from writing out lots of stuff in
[04:29.44] component in coordinate basis
[04:32.32] components so that's why i want to
[04:34.56] introduce this notation and spend a long
[04:36.40] time getting you comfortable with it
[04:37.84] because it'll just be so handy and
[04:39.12] prevent save us from additional
[04:40.88] handwriting in future lectures
[04:44.32] all right so in way two
[04:49.36] we think of
[04:50.96] t as a function
[04:54.84] right
[04:56.56] with
[04:57.68] key
[05:00.24] entries or
[05:02.32] arguments
[05:06.00] from
[05:07.76] vp star or of
[05:10.56] vp star
[05:11.84] and
[05:12.17] [Music]
[05:13.60] it has l
[05:15.28] entries
[05:16.96] or arguments
[05:19.92] from
[05:21.28] of depending on how you like to say it
[05:23.04] vp
[05:24.72] and so if you do if you think about it
[05:26.40] like that t is a function that has these
[05:28.72] arguments
[05:29.92] then the notation you would use to do
[05:31.68] that to describe t now is something more
[05:33.68] like this right you'd say oh it's a
[05:35.04] function that has
[05:36.32] k entry k arguments which i'll denote
[05:39.12] like this with a semicolon
[05:42.16] to delineate between the the
[05:46.16] the vp start arguments and the vp
[05:48.00] arguments so there's k
[05:50.84] entries and there's l
[05:53.04] arguments or entries here so you think
[05:54.64] of t as a function it takes many
[05:56.32] variables right one variable per entry
[06:00.08] and uh
[06:01.84] inside where i've got all these dots
[06:04.24] that's where you place tenses that place
[06:06.32] vectors from vp star and vp
[06:10.72] so you think of
[06:12.16] t as this kind of this this thing here
[06:14.48] and this is a function right and and
[06:16.16] what is it well it's a real number that
[06:17.76] that's the way we're interpreting it and
[06:19.36] so what we're going to do is we sort of
[06:20.48] in place
[06:22.16] we think of this as a machine
[06:26.64] this tensor here is a machine what does
[06:28.48] this machine do
[06:30.88] well
[06:32.84] it that didn't work out um
[06:38.80] yeah let's try it again yeah that works
[06:40.96] better it you can think of this tensor t
[06:43.68] as a machine that eats
[06:45.84] vectors and covectors curvatures and
[06:47.92] vectors and produces numbers it's like a
[06:50.40] little box
[06:53.60] and so in the first entry it takes an
[06:56.32] argument of a type
[06:58.32] vp star of a covector so
[07:00.88] it takes some kind of co-vector another
[07:03.12] one
[07:04.24] all the way up to k and then in the
[07:06.08] second
[07:07.76] arguments it takes vector entries
[07:11.60] so each of these
[07:13.04] omegas
[07:14.24] are elements of vp star they're
[07:16.72] covectors and each of these
[07:19.36] vls are elements of vp they're just
[07:21.68] vectors
[07:24.32] so evaluation of
[07:27.60] t
[07:30.72] is equivalent to just sticking in
[07:32.40] arguments of various types into these
[07:34.40] these entries so it's like a machine
[07:36.64] that eats these uh elements
[07:40.00] you can think of tea as a box right
[07:42.48] it takes k arguments of type vp star
[07:49.60] omega 1 omega 2 all the way down to
[07:52.48] omega k and it takes
[07:54.88] l
[07:58.00] arguments of type vp
[08:01.52] and it produces a number r
[08:05.44] that's a that's another way to think
[08:07.04] about t
[08:09.52] and abstract index notation that's
[08:12.16] that's where we are now
[08:14.16] it's just a way
[08:15.52] to specify
[08:18.64] not the tensor itself this is important
[08:21.60] it's just a way to specify
[08:23.60] the
[08:24.84] type of a tensor
[08:28.48] so the type of a tensor is like how many
[08:31.04] co-vector arguments it has and how many
[08:32.56] vector arguments it has
[08:36.72] and by
[08:38.24] naming i'll get rid of it simply
[08:40.64] by naming
[08:43.44] these entries
[08:45.68] and the notation then makes it clear
[08:47.20] which entry is of which type
[08:49.68] so
[08:50.80] probably the best way to do it is by
[08:52.16] example so let's think about a tensor
[08:54.32] that has three arguments slots or
[08:56.40] entries uh sorry six arguments
[08:59.60] slots or entries
[09:03.12] so it takes
[09:04.40] three arguments of type co-vector and
[09:06.88] three arguments of type vector
[09:08.96] tangent vector
[09:11.28] and we're going to label these entries
[09:13.12] so this a just means the the first entry
[09:15.52] b means the second entry c means the
[09:17.68] third d means the fourth
[09:20.56] e the fifth and uh
[09:23.52] f the sixth entry of this multi-linear
[09:26.00] function from a vector space and this
[09:28.16] goes to the real numbers
[09:29.84] and now we
[09:31.28] specify
[09:33.60] the type and by the type i just mean you
[09:36.48] know how many entries it has of type
[09:38.96] co-vector entry and how many entries it
[09:40.72] has a type vector entry
[09:42.32] uh as follows
[09:46.16] a b c d e f so this these letters here
[09:50.24] are not components right
[09:52.80] this is very important to say that again
[09:57.04] these thingies here these letters
[09:58.72] they're just uh
[10:00.16] names for the the
[10:01.76] entries
[10:05.84] all the arguments
[10:09.36] they're just
[10:10.64] labels
[10:13.12] they label the um
[10:16.64] just the entries so i can talk about the
[10:18.24] b entry right that just means the second
[10:20.32] argument of this multilinear functional
[10:24.56] and so all this says
[10:29.52] what does this say when you look at an
[10:30.96] expression like that what you're meant
[10:32.72] to say is i've got a tensor
[10:37.36] which takes
[10:45.84] takes
[10:47.52] three
[10:49.20] arguments
[10:53.28] from vp star
[10:55.20] and three arguments
[10:59.44] from vp
[11:01.28] it's all when you see so when you see
[11:03.20] something written like that in this
[11:04.48] course with lowercase latin
[11:06.40] letters
[11:07.28] these lowercase letters denote
[11:09.52] entries or arguments for this linear
[11:12.16] function that's all it says
[11:16.08] if you really want to know the actual
[11:17.60] components in a chord in some basis
[11:23.60] the actual components
[11:31.36] of t with respect
[11:35.12] to a coordinate basis
[11:39.28] or coordinate system
[11:48.16] ah
[11:49.04] well we have to expand t as a linear
[11:52.00] combination of all these bases
[11:54.56] coordinate basis
[11:56.24] elements of vp and vp star
[11:59.04] well just look refer to my previous
[12:01.68] lectures now
[12:03.12] ah what are they well you know it's an
[12:05.68] it takes three
[12:07.68] cobact arguments
[12:11.60] three
[12:14.80] vector arguments
[12:18.24] notice i'm now using greek letters to
[12:20.00] index these components
[12:21.84] and we have to have some kind of
[12:23.12] coordinate basis here it is
[12:26.88] this is remember d dx mu 1 is a basis
[12:32.08] coordinate basis
[12:35.36] of v p star
[12:39.92] dxnew one
[12:42.32] and then dx is the dual basis remember
[12:44.96] you're not meant to think of it yet as
[12:46.08] anything other than an element of a
[12:48.40] vector space
[12:49.76] that's the actual components of t with
[12:52.24] respect to a coordinate system in the
[12:54.00] the the corresponding coordinate basis
[12:56.32] that's very important to see the
[12:57.84] difference between these things i've
[12:59.76] here these really are components now
[13:01.84] these are numbers
[13:03.20] right this this stuff that i've
[13:04.72] highlighted here each one of these for
[13:07.12] each
[13:10.56] for each choice of
[13:15.20] of mu
[13:16.32] 1 mu 2 mu 3 new one new two new three
[13:22.24] this is a number
[13:29.76] you know as soon as i input mu1 mu2 mu3
[13:32.80] u1 u2 u3
[13:34.48] then that's a number it's just a number
[13:36.32] or a function right if you're thinking
[13:38.24] about this as a
[13:40.96] as a function in fact let's
[13:43.20] i take that back about the number it is
[13:44.64] a number if you evaluated a point um
[13:49.04] is a function of
[13:50.80] m
[13:53.28] whereas let's go back to this abstract
[13:55.60] index notation there's no such thing as
[13:58.08] saying a equals one b equals one or c
[14:01.12] equals one in this expression these are
[14:02.72] just labels for entries of a
[14:04.24] multi-linear function
[14:08.96] now the
[14:10.72] okay so that's just the notation for
[14:12.48] specifying the type of a tensor so far
[14:16.40] we can but the you know a good notation
[14:18.56] is one which you can
[14:20.08] exploit to
[14:22.16] save you work
[14:23.68] and
[14:24.72] a good notation is one which is sort of
[14:27.04] behaves nicely with respect to certain
[14:28.88] operations
[14:30.56] so it turns out that there's a lot of
[14:32.16] tensor operations that we will do in
[14:33.68] general relativity and also in other
[14:35.36] fields where just knowing the type of a
[14:37.68] tensor
[14:39.36] will allow you
[14:41.20] you can already say a lot by just
[14:43.36] saying specifying the types of tenses
[14:46.48] and you can also perform several
[14:48.08] manipulations just on the types of
[14:50.80] tenses
[14:51.76] the notation this abstract index
[14:53.44] notation will capture these operations
[14:55.52] very succinctly it's a good notation in
[14:57.68] that sense
[14:59.12] so
[15:00.16] certain
[15:03.76] tensor operations
[15:09.12] can be expressed
[15:10.80] succinctly let's say
[15:14.48] or compactly
[15:18.64] with abstract index notation and this is
[15:20.72] why we like in the abstract index
[15:22.24] notation
[15:23.68] so let's just go through a list of the
[15:25.60] operations we know of tensors and see
[15:27.28] which ones can be expressed compactly
[15:29.04] succinctly nicely with respect to
[15:31.04] abstract index notation and and we'll
[15:33.20] avoid those that can't so the first
[15:36.00] tensor
[15:38.16] operation that we will exploit
[15:42.08] abstract index notation to specify
[15:44.48] is that of contraction so if you
[15:46.00] remember
[15:47.20] if you have a tensor of type i'm going
[15:49.60] to try and
[15:50.80] stay consistent with that language but
[15:52.24] i'll probably slip in this course i
[15:53.76] apologize if you have a tensor of type k
[15:56.48] l or meaning that it has k
[15:58.80] co vector arguments k
[16:00.40] l vector arguments so if that is a
[16:02.80] tensor
[16:04.48] then remember the contraction
[16:07.60] of the j
[16:09.12] and the j prime
[16:11.20] uh argument of t
[16:13.84] is defined to be the following tensor
[16:15.84] right
[16:17.28] it's the one where you you insert a
[16:19.60] basis in the jth argument in the the j
[16:22.00] prime argument and then some
[16:24.24] well what kind of ten what what's the
[16:26.32] type of the resulting tensor
[16:29.12] uh well we can
[16:32.56] write out
[16:34.40] the tensor of this type
[16:37.36] it should have one less entry right
[16:39.20] because we we in both the the curve
[16:41.92] vector entries and the vector entries so
[16:43.76] it
[16:45.92] it's gonna have
[16:47.60] the the previous entries are still
[16:50.00] labeled by these latin letters but we
[16:52.16] want to have a notation where
[16:55.60] we uh
[16:58.72] want to indicate that we've contracted
[17:00.64] over a pair of entries
[17:03.20] and the way we will do that is that
[17:05.44] we'll use the letter c we'll repeat the
[17:07.68] letter c this is expanding or extending
[17:09.76] the abstract
[17:12.80] and the first c is
[17:16.48] in the
[17:18.64] jth
[17:19.76] entry
[17:21.36] and the second c
[17:25.44] is in the j prime
[17:27.68] entry
[17:29.44] or replaces the j primes entry so the
[17:32.64] notation the abstract notation abstract
[17:35.28] index notation for contraction is to
[17:37.84] simply write out the type of the tensor
[17:39.60] the replace the entries that you're
[17:40.88] contracting over with a common letter
[17:42.64] and then that now is understood as
[17:44.40] meaning the contraction so this is what
[17:46.32] the
[17:47.20] definition of a notation here
[17:58.84] then that's contraction now let's get to
[18:02.48] uh and okay okay
[18:05.12] what what type of the tensor do we have
[18:14.96] you know what type of tensor do we have
[18:16.32] now well we've contracted out a case
[18:19.04] the jth entry of a cobacter type and the
[18:20.96] j prime entry vector type so we should
[18:22.96] have a tensor of type
[18:27.20] uh k minus 1 l minus 1. and the abstract
[18:31.20] index notation should allow us to
[18:33.52] to see that and it does if you think of
[18:35.36] this repeated c as as not being
[18:37.76] accessible anymore we can't assign to
[18:39.60] the repeated c
[18:41.12] entry
[18:44.40] okay so that's contraction
[18:53.04] i will actually i'll pause here to say
[18:56.16] something as well about abstracting the
[18:58.56] index notation and transformations of
[19:00.48] tensors so
[19:03.68] interlude
[19:07.04] if you have a tensor of type k
[19:09.68] l
[19:11.84] this has in general relativity and
[19:13.44] differential geometry this has extra
[19:14.88] baggage
[19:16.00] this means what this is
[19:18.32] we also demand
[19:21.44] implicitly and have been demanding
[19:23.60] implicitly
[19:26.56] that
[19:27.60] t
[19:28.32] transforms
[19:31.92] as a tensor
[19:36.16] of type kt kl
[19:39.36] under
[19:41.52] a change of coordinate bases
[19:48.48] so that's something that
[19:50.16] is very
[19:51.20] an unfortunate
[19:53.36] consequence of the language that we use
[19:55.12] to talk about tenses is that there's two
[19:56.80] notions of basis and i'm going to try
[19:58.88] and stress this throughout this course
[20:00.56] that the
[20:02.40] these two notions of basis
[20:04.64] uh
[20:06.24] can interfere with each other
[20:07.44] conceptually so the first notion of
[20:09.84] bases well you can think of a tensor
[20:11.28] remember as an element of a vector as a
[20:13.12] vector in a vector space so there's a
[20:14.56] notion of the basis of a vector space
[20:16.56] right now that has uh that that's well
[20:19.28] and good and that's fine
[20:21.20] but there's another sense in which a
[20:22.88] tensor um
[20:26.00] uh is defined with respect to a basis
[20:28.48] and that is a tensor is also
[20:30.96] in differential geometry isn't just an
[20:33.04] element of this vector space i mean
[20:34.40] there's a vector space for each point on
[20:36.08] the manifold right
[20:38.00] so a tensor is really an element of a
[20:40.64] so-called bundle right so a t
[20:45.04] of type
[20:46.84] kl is really
[20:50.72] in differential geometry and general
[20:52.56] relativity
[20:54.16] and infinite
[20:56.00] i mean i've said it before but i i think
[20:57.84] it's worth stressing several times list
[21:00.88] of tenses
[21:04.24] you know one for each
[21:07.92] point p in the manifold
[21:11.60] so that's a really important point in
[21:13.68] moving forward in general relativity
[21:15.68] when you have a tensor when we talk
[21:17.84] about the tensor in general relativity
[21:19.36] we're not talking just about that list
[21:21.44] of numbers that vector in one vector
[21:24.24] space but we're really thinking about
[21:25.76] these vector spaces
[21:27.28] being all assembled together in a circle
[21:29.76] tangent bundle right
[21:32.88] and so
[21:33.84] t is a function if you like uh
[21:38.56] well yeah it's a list of functions one
[21:40.88] per point in the manifold so that's an
[21:42.88] important interlude in the abstract
[21:44.64] index notation we're implicitly
[21:46.24] demanding that not only is t a tensor
[21:48.48] with respect to one point on the
[21:50.24] manifold as a tensor with respect to
[21:51.76] every point on the manifold and
[21:53.12] transforms accordingly
[21:57.60] and that that means you have to do a
[21:58.64] little bit of work uh
[22:01.04] trying to convince yourself uh that a
[22:03.68] tensor is a tense in the sense of
[22:05.12] general relativity i mean any element of
[22:07.28] a tensor product vector space is a tense
[22:08.72] in the sense of vector spaces but in
[22:10.24] general relativity we have a list of
[22:11.84] tensors one per point on the manifold
[22:14.32] right
[22:16.40] and that's why you have to work harder
[22:18.24] in differential geometry and
[22:21.92] to prove that something is a tensor
[22:25.52] you know for every point on the manifold
[22:28.16] t is defined
[22:32.16] there's an infinite
[22:34.32] list of t's one for each point in the
[22:36.40] manifold
[22:38.32] everywhere you look on the manifold
[22:39.92] there's a point
[22:41.44] and
[22:43.04] around that point
[22:44.72] or at that point so we're not around at
[22:46.72] that point each of these
[22:49.92] points there is a tensor
[22:54.88] and
[22:55.92] what constitutes a good list of
[22:58.56] tenses well they have to transform
[23:00.08] nicely when you change coordinates right
[23:02.00] and that's this tensor transformation
[23:03.28] law that we covered in the previous
[23:04.48] lectures
[23:06.48] so in abstract index notation we're
[23:08.16] implicitly talking always about a
[23:09.76] manifold there's always a manifold in
[23:11.44] the background and when we say the word
[23:13.36] tensor we're talking about not just one
[23:15.28] element of vector space but a list of
[23:17.60] elements of these vector spaces one for
[23:19.20] each point in the manifold
[23:22.16] okay back to abstract index notation
[23:24.08] let's see how how can we express outer
[23:26.32] products
[23:27.92] specified tenses
[23:29.68] uh the other products of tensors with
[23:31.28] abstract index notation it also plays
[23:33.36] nicely the notation
[23:35.84] plays nicely without a product so if you
[23:37.68] have
[23:39.12] a tensor of type k plus l
[23:44.88] so if t is in
[23:47.20] that vector space and
[23:49.28] this is a tensor of type k prime
[23:52.32] l prime
[23:59.76] suppose we have these two tensors of
[24:01.12] this type
[24:02.56] k l and k prime l prime then
[24:05.20] the
[24:06.08] a abstract index notation for the outer
[24:08.56] product
[24:12.48] what kind of tensor is it what's the
[24:14.16] type of the tensor of an outer product
[24:15.84] which tends to just add up the the types
[24:17.84] right
[24:19.52] is
[24:20.88] so whenever you see
[24:22.24] this concatenation of
[24:27.92] of
[24:29.44] abstract index notation
[24:31.36] to find tensors then this just means
[24:33.84] take their outer product
[24:37.36] so the outer product is the absolute
[24:39.76] index notation for the outer product is
[24:41.60] defined
[24:43.92] you can't derive this it's just defined
[24:46.00] right this is we're defining the
[24:47.28] notation
[24:49.60] and that's just looking counting the
[24:51.44] upper uh the arguments of the upper
[24:53.12] indexes you know how many
[24:55.04] how many upper industries are those well
[24:56.40] there's k plus k prime of them and this
[24:58.40] l plus l prime of the lower indices and
[25:00.24] therefore we already know what the type
[25:01.44] of the tensor is and we can talk about
[25:03.20] their entries
[25:07.28] let's give an example
[25:09.04] moving on with contraction um
[25:12.08] to to
[25:13.44] to get to more general relativity
[25:15.04] specific operations now we've got to
[25:16.48] introduce the metric remember the metric
[25:18.08] is a tensor of type zero comma two
[25:20.80] uh
[25:22.32] so g is a tens
[25:24.24] if you have you know if g is a metric
[25:27.36] then what is its abstract index notation
[25:30.16] well you've got a tensor with two vector
[25:33.92] arguments so you've got to label them
[25:35.92] with latin letters there you go that's
[25:37.20] i've now specified a tensor of type zero
[25:39.36] two
[25:41.12] we kind of overload uh g is always the
[25:43.52] metric right this is g is always the
[25:45.44] letter for the metric so this is that's
[25:47.44] the abstract index notation for a metric
[25:49.84] so if you have a tensor
[25:51.60] of type zero two then you just specify
[25:53.84] it by putting two lowercase latin
[25:55.44] letters where
[25:57.04] expanding the abstract index notation
[25:59.28] here
[26:00.48] to
[26:02.32] the special case of metrics
[26:04.88] is hardly a special case of the notation
[26:06.88] we're just literally using the same
[26:08.24] notation however when you see the letter
[26:10.72] g you must always think metric in this
[26:12.56] course
[26:15.04] so that's the sense in which this
[26:16.32] notation is being
[26:17.84] further
[26:19.68] enlarged here
[26:21.68] so g means metric all right a metric is
[26:24.16] not in the intensive type zero two it's
[26:25.84] going to be non-degenerate and blah blah
[26:28.00] blah
[26:32.40] okay so that's the abstract index
[26:33.76] notation for a tense a metric tensor
[26:36.40] just g so that's actually the same
[26:38.56] notation
[26:39.84] but now
[26:41.60] uh we have something very interesting
[26:45.44] to do with the metric uh namely we can
[26:47.92] raise in lower indices
[26:49.92] with a metric
[26:51.36] and that's because
[26:53.20] uh
[26:54.64] g
[26:55.76] transforms and its inverse in a
[26:57.84] particularly nice way
[27:00.40] so in fact before raising and lowering
[27:02.08] let me talk about the inverse of g
[27:08.80] so the inverse of g
[27:12.08] so g is
[27:13.44] uh you can think of g as a matrix
[27:16.16] and
[27:17.04] we've seen that before
[27:22.80] namely by contracting
[27:24.56] the second argument with a vector
[27:28.08] think of g as a matrix
[27:30.24] form
[27:32.16] its inverse matrix
[27:36.64] and you can
[27:38.56] determine that this is a tensor of type
[27:40.72] two comma zero right so the inverse of a
[27:43.52] tensor is actually a tensor transforms
[27:45.92] as
[27:47.76] a tensor of type two comma zero rather
[27:49.76] than type zero comma two
[27:52.88] so that's an exercise just confirm for
[27:54.56] yourself that if you form the
[27:56.40] metric tensor as a matrix find its
[27:59.04] matrix inverse
[28:01.20] i didn't want to write the word matrix
[28:02.96] here in fact i think that's probably
[28:03.92] very important
[28:05.52] form its matrix
[28:08.24] inverse
[28:13.28] all right form the matrix inverse of g
[28:15.76] denote that is g minus
[28:17.60] that's a tensor of type 2 0 right that
[28:19.68] requires a calculation this is not
[28:21.20] obvious
[28:24.00] and then
[28:25.76] now you've got a tensor of type 2 0.
[28:28.08] what's the abstract index notation we
[28:29.84] should use for that well the a the
[28:32.00] abstract index notation according to the
[28:34.00] rules i've given you so far is that you
[28:35.92] should denote that as g minus one a b
[28:39.28] right that's what that's
[28:42.08] that that's the correct use of the
[28:44.32] abstract index notation but we're going
[28:46.40] to be we're not going to do that in this
[28:48.48] course
[28:49.60] we'll
[28:50.48] make also use this notation in the
[28:52.88] special case that we have g
[28:54.80] to refer to g inverse so this actually
[28:57.52] whenever we have g a b
[28:59.52] with upper latin letters that means
[29:02.96] uh form the inverse
[29:05.44] of g
[29:07.44] so it's a tensor of type looking at the
[29:09.36] letters what's the type what's got two
[29:10.88] upper letters now so it's got to be two
[29:12.24] type two comma zero
[29:17.04] now uh we can do things like raising and
[29:19.76] lowering
[29:24.24] of
[29:25.60] of tensor arguments
[29:28.24] so
[29:28.96] let's
[29:31.52] think of the following
[29:33.68] example let's apply the the metric g to
[29:36.48] a vector v
[29:40.56] how would we do that
[29:44.16] in abstract index notation what's the
[29:45.76] definition for these operations well in
[29:47.36] abstract index notation the metric is
[29:49.04] denoted gab
[29:51.04] and a vector in abstract index notation
[29:53.92] is denoted v superscript a it's a vector
[29:56.80] it's an element of type uh one zero
[30:00.56] and
[30:01.60] applying a tensor to a vector or a
[30:04.08] matrix to a vector is a
[30:05.92] is actually a sequence of two operations
[30:07.92] form their outer product is the first
[30:09.52] operation and then contract is the
[30:11.68] second operation
[30:16.96] so step one is to form the outer product
[30:19.76] and that's uh
[30:22.64] g tens of v but in abstract index
[30:25.28] notation the outer product of two tenses
[30:27.36] is denoted in this way here you just
[30:29.44] concatenate the abstract index notations
[30:32.08] and then the second part of matrix
[30:33.68] multiplication or
[30:38.40] matrix multiplication is contraction
[30:42.56] and that is
[30:44.32] indeed in this case here we have to take
[30:46.72] our tensor
[30:48.24] g tensor v and contract the second and
[30:51.20] the third
[30:54.08] arguments of this new tensor
[30:58.72] and then that's the
[31:00.08] end result of that is
[31:02.16] indeed a
[31:03.60] vector
[31:09.36] sorry a co-vector
[31:13.12] now let's do this in abstract index
[31:14.64] notation
[31:17.20] now i told you that contraction means
[31:19.52] you you repeat the latin letter and that
[31:21.60] means you should interpret this object
[31:23.20] now as a
[31:24.48] as a contracted version of the previous
[31:26.56] object
[31:27.92] so that whole operation in abstract
[31:29.44] index notation looks like g a c v c
[31:33.20] and looking at the type we've got this
[31:34.56] repeated latin letter that's
[31:36.32] now denotes an argument that has been
[31:37.84] contracted we're not allowed to assign
[31:39.28] any elements into that argument or entry
[31:41.12] of this object
[31:42.72] and so we can use that to define a new
[31:45.76] object with just one
[31:47.60] in the abstract index notation it would
[31:49.44] have only one entry and therefore that's
[31:51.44] of type t 0 comma 1. but look at this
[31:54.56] right we've taken
[31:56.24] this by this sequence of
[31:58.84] operations we've gone from an object of
[32:01.84] type vector which is a tensor of type
[32:03.76] zero
[32:04.80] one comma zero to an object of type
[32:07.28] covector which is an uh element of t
[32:09.92] zero comma one
[32:12.56] and that that's called raising and
[32:14.72] lowering so
[32:16.56] uh in fact i'll change the notation
[32:18.24] instead of w i'll literally use v again
[32:20.48] so this is the notation for having
[32:22.24] applied a metric to a vector
[32:24.72] is to lower the the actual notation we
[32:27.12] use to denote that operation isn't
[32:28.96] lowering the letter okay
[32:30.88] so this is
[32:32.88] v a maps to v
[32:34.96] a lower
[32:36.24] really really stands for
[32:41.04] the sequence of operations where you go
[32:42.96] from
[32:44.08] uh
[32:44.88] g
[32:46.40] and v
[32:47.92] you're to it
[32:50.32] then you form the outer product
[32:53.28] and then you contract over entries two
[32:55.20] and three
[32:57.44] that's what that that just this simple
[32:59.28] notation there stands for the sequence
[33:01.44] of three steps
[33:04.24] and that's why we like we like to do
[33:05.68] this abstract index notation is very
[33:07.52] helpful we like to employ these kind of
[33:09.52] notations to save us from writing our
[33:11.76] long repetitive steps of the same kinds
[33:14.48] of operations
[33:15.92] now uh now you've seen this like let's
[33:18.72] just check does this play nicely with uh
[33:22.24] with things
[33:23.44] with with the tenses and uh other tenses
[33:26.56] and does it generalize nicely is it well
[33:28.80] defined like can we get into trouble
[33:30.40] with this notation
[33:32.72] well let's just write down a sequence of
[33:34.48] lemmas
[33:35.52] so for example
[33:36.96] if we
[33:38.40] form the following
[33:40.64] outer product and contraction of two
[33:42.40] metric tenses
[33:43.92] then uh we should get the chronic delta
[33:47.28] tensor
[33:49.36] well
[33:51.04] you know what does this
[33:54.24] notation even mean here so this notation
[33:56.80] means okay we have two we have g inverse
[33:59.68] we have g
[34:00.80] we form the outer product
[34:02.72] and then we contract
[34:05.44] over the second and third entries
[34:09.04] that's what this notation here means
[34:11.28] and that means that we're forming the
[34:13.84] matrix
[34:15.60] product
[34:17.84] of g and g inverse
[34:20.72] and the matrix product of the matrix and
[34:23.20] its inverse is just the delta is the
[34:24.96] identity
[34:27.12] and the identity
[34:29.12] in abstract index notation
[34:32.00] is actually uh delta a
[34:35.04] b
[34:36.24] or ac in this case
[34:39.04] so that's that's a little way of arguing
[34:41.36] that this sequence of operations
[34:44.08] outer product or first inverse outer
[34:46.32] product contract gives you something
[34:49.20] that you you would hope
[34:50.72] that you would expect from ordinary
[34:52.24] matrix arithmetic
[34:57.68] okay we can generalize this notation
[35:05.60] generalize this raising and lowering
[35:08.72] so we're just defining notation here
[35:10.88] these notational steps
[35:14.32] when you see them you have to unpack
[35:16.96] them or unzip them
[35:19.12] into actual operations that you perform
[35:24.16] so let's suppose that
[35:27.04] t is a type 33
[35:29.52] so i've labeled this the six entries of
[35:31.68] t it's abcdef in the absolute index
[35:34.08] notation
[35:35.20] um form
[35:37.60] the outer product of g with t and then
[35:39.84] see what happens form g
[35:42.40] and then contract over the second and
[35:45.44] third arguments of g times t
[35:48.32] what is this in abstract index notation
[35:50.48] well we've now defined the notation far
[35:52.08] enough that we can do this
[36:00.16] this is now the definition
[36:02.24] of this abstract index notation
[36:07.36] whenever you form the outer product of a
[36:09.84] metric
[36:10.72] and another tensor then contract over
[36:12.64] one of the arguments of the metric
[36:13.84] that's called raising or lowering we
[36:15.20] define that up above just in the case of
[36:16.80] vectors we're now extending it to
[36:18.00] tensors
[36:19.12] and you pull down the corresponding
[36:20.56] latin index that's the notation for that
[36:22.16] sequence of operations and just looking
[36:24.48] at the type of that tensor you see it
[36:26.08] has four lower case latin that is in the
[36:28.16] bottom and two upper so it's got to be
[36:30.00] of type 2
[36:32.00] 4 t24
[36:35.60] and the nice thing about this is this
[36:37.12] notation is consistent with repeated
[36:39.28] applications of g and g inverse
[36:43.68] so it plays nicely together still we
[36:45.44] haven't found any contradictions in the
[36:46.96] notation
[36:49.52] i want to stress again that all we've
[36:51.36] been doing for the first half an hour of
[36:53.52] this lecture is defining a notation
[36:56.16] this notation is meant to stand for
[36:57.92] actually well-defined operations
[37:08.56] so repeated applications
[37:13.84] of g
[37:14.96] and g inverse
[37:16.64] so for example
[37:18.24] if we do g and g inverse a couple of
[37:20.32] times we should the notation should give
[37:22.80] us allow us to cancel uh repeated
[37:25.60] applications right
[37:27.28] so let's just do an example of something
[37:28.96] like that so let's form the following
[37:31.04] tensor via contractions and
[37:35.44] outer products
[37:38.80] so in the abstract index notation we're
[37:40.64] told that we should replace this
[37:42.96] double application of g
[37:46.00] uh
[37:47.20] and g inverse with a chronic delta type
[37:50.08] function
[37:52.96] and that means that entry a is the same
[37:55.12] as entry a double prime oops should be
[37:57.44] double prime up there
[37:58.88] and then that's defined to be just t a b
[38:01.92] c d e f okay so
[38:06.40] this double application of g and then
[38:08.08] followed by g inverse
[38:10.08] is left invariant uh in the abstract
[38:12.24] index notation and that matches with
[38:14.08] what we uh
[38:17.04] with the notation or rewriting rules
[38:19.04] we've introduced for abstract index
[38:20.56] notation here
[38:24.48] we can extend the abstract index
[38:26.16] notation also to cover a variety of
[38:28.48] arithmetic operations with tensors
[38:30.88] namely addition and subtraction that
[38:32.96] seems easy enough right the notation
[38:35.20] should play nicely together
[38:37.36] and indeed it does so if t and t prime
[38:40.32] are tenses of type kl
[38:42.64] then t plus t prime
[38:45.36] is also a tensor of type
[38:48.08] kl no worries and then if we use the the
[38:53.28] abstract index notation we can define
[38:55.36] the abstract index notation
[38:57.60] for the sum to literally be
[39:00.64] this this notation here
[39:04.32] so this just defines the abstract index
[39:06.24] notation just take the sum of the
[39:08.32] abstract index notations so whenever you
[39:10.40] see that that just means take the sum of
[39:12.24] the corresponding tenses and in this way
[39:14.88] we can actually
[39:16.72] describe uh we can now start to describe
[39:19.60] interesting subspaces of of tensors
[39:22.80] here's two subspaces that will play a
[39:24.56] very important role in general
[39:26.00] relativity
[39:27.28] now we're going to define if
[39:29.36] some more abstract index notation
[39:32.56] whenever you now henceforth whenever you
[39:34.40] see this notation round bracket list of
[39:36.88] indices close round bracket that means
[39:39.68] take the symmetric average
[39:45.12] that's this
[39:46.80] so it's now possible to i'm defining
[39:49.20] this notation here
[39:50.96] t round bracket a b close round bracket
[39:53.68] that means form the tensor t a
[39:56.88] b the standard the input tensor but then
[39:59.52] form the tensor where you swap the
[40:01.04] entries
[40:02.96] and uh
[40:04.80] and then add them up and take the
[40:06.48] average so that's the abstract index
[40:08.56] notation
[40:09.76] for symmetric sum
[40:12.88] and then we have anti-symmetric sum
[40:15.12] which is defined to be the same thing
[40:17.36] but with a minus sign
[40:22.84] and uh this notation we really will use
[40:27.76] somewhat frequently in general
[40:29.04] relativity it's quite often that you
[40:30.96] want to take a tensor
[40:33.04] t and swap around its
[40:36.96] entries
[40:39.52] remember uh so
[40:41.60] here's sort of an example of this
[40:43.04] phenomenon going on suppose we have a
[40:44.88] tensor of type three three it has six
[40:47.68] entries six latin letters denoting the
[40:49.68] entries remember it's a kind of box the
[40:52.08] there's the eighth entry the b entry the
[40:54.40] seat entry
[40:55.84] and then uh
[40:58.00] co-vector entry then we've got d e and f
[41:01.44] all right these each argument here
[41:04.24] is denoted with this notation and now
[41:06.64] you can talk about
[41:08.08] um a b
[41:09.76] uh
[41:10.64] for example b a
[41:12.64] c d e f
[41:16.24] uh if you making with reference to the
[41:18.24] first tensor what does this mean well
[41:19.76] this means exactly this tensor here but
[41:21.28] you swap over these
[41:23.20] these entries
[41:32.80] so if you've got the first tensor
[41:34.32] hanging around
[41:36.56] notationally there
[41:38.40] then
[41:40.80] you can define the swapped one where you
[41:42.40] just you know take the first argument
[41:44.08] stick it in the second argument and the
[41:45.68] second and the first by just swapping
[41:46.96] the corresponding latin letters
[41:48.88] so by itself both of these tenses are
[41:51.44] tenses of type 3 3 but when you have
[41:53.76] both of them in an expression you it
[41:55.68] means that you have to
[41:57.20] follow where the entry goes
[42:02.08] so just be aware
[42:04.48] that we the the the swapping notation
[42:07.12] now going back to this subspace of
[42:09.76] symmetric and anti-symmetric tenses we
[42:11.60] can define
[42:12.64] further uh
[42:15.36] tensor operations
[42:20.96] analogously so so we
[42:23.12] define for now a tensor of type 0k
[42:29.76] takes k vector arguments we're going to
[42:31.68] define a new operation very very handy
[42:34.48] operation
[42:36.08] uh by just extending the notation that i
[42:38.24] just introduced above so just the round
[42:40.08] bracket thing again
[42:41.84] what does this round brackets mean now
[42:43.20] well it means now you sum over all
[42:44.72] permutations
[42:47.36] of those k letters
[42:55.68] okay so you form the tensor
[42:59.20] which is the average of a k factorial
[43:01.60] permutations of all those k entries here
[43:04.16] so sk is the symmetric group of
[43:06.64] permutations
[43:19.04] of k entries or letters objects
[43:23.84] and how many elements does that group
[43:25.52] has l k factorial well i've written l
[43:28.16] there so that's a mistake
[43:31.68] k factorial entries there
[43:36.00] and then we can also expand that square
[43:38.56] bracket notation so
[43:40.72] if you have square brackets
[43:43.84] oh i see
[43:45.76] okay wait on
[43:47.28] pause for a moment go back make this
[43:49.28] notation consistent sorry about that so
[43:52.24] i tend to like having l's don't i for
[43:55.36] for lowercase
[43:56.96] entries
[44:05.44] so
[44:07.84] if you have a tensor of type zero l it
[44:10.08] means it takes l vector entries and you
[44:12.96] uh
[44:13.76] use this round bracket abstract index
[44:15.36] notation that means that stands for
[44:17.84] take the tensor and all its l all l
[44:20.40] permutations of its entries add them up
[44:22.16] and divide by l factorial that's what
[44:24.00] that notation means correspondingly
[44:28.00] if we have square bracket notation
[44:31.12] that means do pretty much the same thing
[44:34.40] but now
[44:36.64] weight the sum by plus or minus one
[44:38.96] depending on whether the permutation is
[44:40.48] order or even
[44:45.92] oops not k but in fact l
[44:48.88] so there's a new notation here this this
[44:51.04] thing here is a completely any symmetric
[44:52.96] symbol
[44:56.24] so
[44:57.52] e of pi equals plus one
[45:03.20] if pi
[45:04.40] is a product
[45:07.76] of an even
[45:08.74] [Music]
[45:10.24] number
[45:11.68] of transpositions
[45:15.20] minus one otherwise
[45:19.92] okay so we can form these two uh
[45:22.32] tenses you know given it and just any
[45:24.56] old tense so we can form these
[45:25.92] symmetrized and anti-symmetrized tenses
[45:31.52] and the abstract index notation stands
[45:33.84] for these these operations
[45:36.48] so we can mix the notation
[45:42.56] it's also pretty cool
[45:44.72] consequence of this so uh
[45:47.68] if you have a tensor type
[45:49.60] 3 2 for example
[45:53.36] then we can form the following tensor
[45:54.96] you can oops you can
[45:57.36] symmetrize over the first two indices
[45:59.28] leaving the third one alone and then
[46:00.96] anti-symmetrize over the last two
[46:06.80] and what does that stand for it stands
[46:08.16] for the following tensor right you've
[46:09.84] got to symmetrize over the first two
[46:12.88] then anti-symmetrize
[46:15.20] over the last two entries
[46:21.04] okay
[46:23.28] and finally a bit of notation
[46:26.48] he
[46:27.44] totally anti-symmetric
[46:38.24] tensor
[46:39.52] of
[46:40.32] type
[46:42.32] zero k
[46:43.92] right
[46:47.12] a totally any symmetric tensor of type
[46:49.20] zero l sorry keep using k
[46:54.64] um i.e what is a totally anti-symmetric
[46:56.80] tensor now here's a definition right a
[46:58.80] totally anti-symmetric tensor is one
[47:00.64] who's
[47:01.68] which is invariant under
[47:03.04] anti-symmetrization
[47:06.40] and a totally symmetric tensor therefore
[47:08.08] is one that's symmetric uh invariant
[47:10.32] under symmetrization
[47:12.08] these are these have a special name
[47:13.60] they're called
[47:15.68] differential l forms
[47:25.04] and such a differential l form plays an
[47:27.20] important role in trying to define
[47:28.56] integrals in manifolds we're not going
[47:30.64] to get to that yet
[47:33.84] so the summary of all of this so far is
[47:36.24] that abstract index notation is a way to
[47:38.32] capture coordinate basis independent
[47:40.72] manipulations without needing to go to
[47:42.72] components coordinate components
[47:45.44] of course a general computation will
[47:46.96] require a coordinate basis
[47:50.88] because
[47:51.84] in a general computation you're going to
[47:54.40] want to get some answers and so
[47:57.44] there's a limit to how much you can do
[47:58.96] without getting any numbers
[48:02.96] so that was the first topic for today's
[48:04.40] lecture the second one now we're going
[48:06.80] to
[48:07.68] move on now with our discussion of the
[48:10.40] differential geometry foundations
[48:12.56] required to to formulate einstein's
[48:14.96] theory of general relativity and we're
[48:16.88] going to move on to the
[48:18.72] quantifying the notion of curvature
[48:21.12] right the curvature plays a very
[48:22.80] important role in general relativity and
[48:25.28] we want to make sure
[48:26.72] that we can precisely uh quantify that
[48:29.60] notion mathematically rigorously
[48:33.04] um and
[48:34.40] uh so that we can then equate curvature
[48:38.08] to the density of stress energy
[48:42.80] so we got to start with
[48:45.04] you know original motivation for the
[48:47.28] getting into manifold theory at all and
[48:49.04] that is that uh
[48:51.52] space-time
[48:53.84] is uh not embedded as far as we
[48:57.60] want to consider in in physics
[49:01.28] we i mean you could always think of
[49:03.04] space time as embedded in some larger
[49:04.80] dimensional manifold
[49:06.48] but we choose not to do that
[49:08.56] and there are reasons to not want to do
[49:10.72] that um especially when you come to
[49:13.52] to thinking about quantum theories of
[49:15.04] gravitation
[49:16.48] but okay let's just take it as a given
[49:18.72] in this course that space time is not
[49:20.08] embedded in some bigger manifold
[49:22.00] therefore we want
[49:27.60] intrinsic we want an intrinsic
[49:31.04] notion of curvature of some manifold
[49:35.68] because if we don't have i mean the the
[49:37.84] point is right it's easy
[49:39.92] you know
[49:41.92] if m were embedded
[49:46.16] in some bigger uh
[49:49.28] flat
[49:51.44] euclidean space or lorenz komski space
[49:54.24] if and were embedded then it is easy
[50:00.56] to see
[50:02.00] if m is curved or has curved curvature
[50:05.60] right i mean it's not totally trivial
[50:07.52] but but
[50:08.48] you know you you'll know it when you see
[50:10.00] it right if you have an embedded
[50:11.20] manifold then you know here's a flat
[50:13.52] manifold and my cartoon of a flat
[50:15.84] manifold embedded in r3 so here's you
[50:18.08] know m
[50:19.12] this is somehow embedded in r3
[50:25.68] versus right you know if you saw that
[50:28.08] that's a that's flat that's not curved
[50:30.40] versus this one
[50:31.92] embedded in r3
[50:37.36] right it's clear
[50:38.72] the second one is is is is curved has
[50:41.44] curvature in this the first one does not
[50:43.44] that was easy right how hard was that
[50:45.60] not hard but if you are stuck if you're
[50:48.56] intrinsically stuck in your manifold m
[50:50.80] and you're not allowed to look outside
[50:52.24] of the manifold in the embedding space
[50:55.04] the ambient space then how would you uh
[50:58.00] how would you
[50:59.20] come up with a criterion a coordinate
[51:01.20] basis criterion
[51:02.96] a coordinate basis independent criterion
[51:05.04] for whether or not the manifold you're
[51:06.80] in
[51:07.68] inside of our space prime whether it's
[51:09.44] curved or not so you need a proxy for
[51:12.64] curvature
[51:18.56] we need some way of quantifying
[51:20.24] curvature
[51:22.48] intrinsically
[51:24.64] without making reference to the manifold
[51:26.88] that uh
[51:27.84] the the ambient flat space that this
[51:29.84] manifold might be embedded in which we
[51:31.44] assume it isn't
[51:34.08] and you can you know here's this mental
[51:36.48] gymnastic you play all the time in
[51:37.92] differential geometry you notice that a
[51:40.16] curve manifold has various features and
[51:42.48] then you try and find the feature that
[51:44.08] can be uh formulated intrinsically and
[51:46.32] then you def use that as your definition
[51:48.24] of curvature
[51:49.68] so you can spend all day thinking about
[51:52.48] various ways in which the curved man
[51:54.72] unfold on the right here is different
[51:56.32] from the flat manifold on the left
[51:58.96] i'll save you the the trouble
[52:01.68] by
[52:03.52] introducing a characterization of
[52:05.04] curvature which is intrinsic um and uh
[52:10.00] and does capture the the notion of
[52:12.08] curvature correctly
[52:15.36] so
[52:16.48] what's the way to do it well
[52:18.48] the way that was successful in
[52:20.48] differential geometry
[52:22.24] is to capture curvature
[52:28.64] by noticing
[52:30.48] right the key is to notice
[52:34.80] that in flat planes
[52:38.32] in flat manifolds
[52:44.08] we can move vectors around
[52:47.28] in a parallel fashion independent of
[52:49.68] paths
[52:53.68] we can capture correlation
[52:56.00] we can capture curvature by noticing
[52:58.00] that in flat manifolds we can move
[52:59.60] vectors around
[53:08.16] in a parallel fashion
[53:15.84] independent
[53:19.04] of the path
[53:20.56] chosen
[53:24.80] so of the many features that
[53:26.32] characterize flat
[53:28.40] euclidean space that's the one that
[53:30.56] we're going to isolate as
[53:33.52] as the an intrinsic notion or as the
[53:35.36] basis for an intrinsic notion of
[53:36.64] curvature
[53:38.24] we can in flat manifolds we can move
[53:41.28] vectors around
[53:42.88] in a parallel fashion
[53:48.80] independent of the path chosen
[53:51.36] so there's a couple of words here that
[53:52.64] we have to talk about i'll just draw a
[53:54.00] picture to define these notions
[53:56.88] here's my cartoon of flat space
[54:00.16] suppose we want we have we're at p we
[54:02.40] want to get to q
[54:04.72] we have a nice tangent vector at p
[54:07.12] and here's a path
[54:08.80] from p to q
[54:10.56] now it's easy to talk about in flat
[54:12.16] space moving a vector around in a
[54:14.24] parallel fashion right you just take
[54:15.76] your vector move it along the path
[54:18.24] and insist that at each point along the
[54:20.16] path the vector is the same vector right
[54:22.08] points in the same direction that's easy
[54:24.32] because in r in flat space you can
[54:26.64] translate vectors and compare them
[54:30.48] two points p and q and we have a vector
[54:32.64] at p then we can talk about moving p
[54:34.96] along a path let's call this part of
[54:36.56] gamma
[54:37.76] in as parallel away as possible so at
[54:41.20] each point on the path you insist that
[54:42.72] the vector on the path is simply the
[54:44.24] same one right and how do you see if
[54:46.16] it's the same one you just translate the
[54:47.52] vector back because rm also has this
[54:50.00] group structure that allows you to
[54:54.80] move vectors around
[54:56.72] and
[54:57.52] if you chose a different path
[54:59.60] gamma prime
[55:01.44] and did the same thing
[55:03.52] insisting that your vector a parallel
[55:05.92] your transport you transport your vector
[55:07.44] around this parallel fashion as possible
[55:11.36] then you get the same answer at the end
[55:13.92] that's something special about flat
[55:16.08] space i mean amongst all the many things
[55:17.84] that are special about flat space that's
[55:19.44] one of them
[55:21.68] and
[55:22.96] on curved
[55:24.56] let's see how good this notion is
[55:27.68] at capturing curvature
[55:29.76] well
[55:30.80] if you have an embedded manifold
[55:33.04] eg a sphere surface of a sphere
[55:39.92] here's a sphere
[55:42.32] and suppose you pick a point p
[55:45.36] and you want to go to the north pole at
[55:47.20] q well if you chose one path right this
[55:50.88] path gamma on this
[55:52.88] great circle here for example and you
[55:54.64] have a vector at p and you move this
[55:56.56] vector as parallel way as possible
[55:59.76] along this because your embedded space
[56:01.92] you can do this you can always translate
[56:04.24] that vector to p through the
[56:07.04] ambient space
[56:08.48] then you you'd end up with a vector at
[56:10.24] the north pole pointing to the right in
[56:11.84] this picture
[56:12.96] but had you chosen a different path
[56:15.52] you know first you go around the equator
[56:17.68] and then you go up to the north pole
[56:20.00] and you parallel transport your vector
[56:22.40] along this path
[56:28.84] um always insisting that the vector
[56:31.68] along the path is parallel to the
[56:33.76] previous vector
[56:36.48] then you'd end up with a vector at the
[56:38.56] end pointing in another direction
[56:41.68] and you're able to do this you're able
[56:43.20] to compare vectors at different points
[56:44.64] because you've got this ambient space
[56:46.48] that you can move the vectors along the
[56:48.16] path to compare them with
[56:51.12] compare each of them with
[56:53.04] and so this idea of parallel
[56:55.28] transporting
[56:56.48] a vector
[56:58.48] in curved embedded manifolds
[57:01.12] can uh detect curvature of the manifold
[57:04.48] because it the curvature shows up as the
[57:06.72] dependence of the parallel transit
[57:08.80] transported vector on the path
[57:13.84] so just to go back here and emphasize if
[57:15.84] you have two points p and q and you
[57:17.92] parallel transport a vector along the
[57:19.68] path
[57:20.56] you can talk about the vector being
[57:22.40] parallel to itself along the path
[57:25.44] because you're able to compare the
[57:26.96] vectors at infinitesimal near nearby
[57:29.20] points just by sliding them along the
[57:30.96] path in the ambient spacetime and
[57:33.04] comparing them
[57:35.76] and and the same goes uh
[57:38.24] in this curved example here
[57:41.68] so curvature
[57:45.04] the the notion of curvature that that
[57:47.04] we're abstracting away here is the
[57:49.28] dependence
[57:52.88] on path chosen
[57:56.40] of parallel transport
[58:01.60] it's this notion that we're going to try
[58:02.96] and gen form an intrinsic version of and
[58:05.52] then generalize to general manifolds
[58:11.36] so
[58:12.88] what in order to to to do that we need
[58:15.44] to define you know we haven't it's not
[58:17.84] easy yet right we have to introduce a
[58:19.36] notion of what is parallel transport on
[58:21.68] a manifold right when we don't have
[58:23.36] access to this ambient space to move the
[58:25.12] vector through
[58:28.56] because the vector in both of these
[58:30.32] examples here the vector
[58:32.88] can be thought of as pointing
[58:35.12] into the the ambient space so
[58:37.44] particularly in the case of the sphere
[58:38.80] right you can talk about the vector
[58:40.96] extrinsically as a vector that's sort of
[58:43.44] in the ambient space and pointing out
[58:46.56] out of the manifold itself
[58:50.08] now we've successfully defined a notion
[58:52.40] of vector uh for
[58:54.64] manifolds so maybe it's not going to be
[58:56.56] so difficult to just introduce a notion
[58:58.56] of parallel transport
[59:00.48] in an intrinsic notion for parallel
[59:02.40] transport
[59:11.04] so let m
[59:12.48] be a manifold
[59:14.96] now we've forgotten that we have an
[59:16.56] ambient embedding space
[59:21.76] so let me a manifold but let's not give
[59:23.84] it any additional structure you know
[59:25.52] it's just make it a manifold with no
[59:27.36] additional structure
[59:34.24] okay
[59:35.04] it's impossible to define a natural
[59:37.28] notion of parallel transport
[59:47.44] okay impossible's a strong word maybe
[59:49.20] some clever person will find some notion
[59:51.04] that captures some
[59:52.80] aspects of parallel transport
[59:55.28] but
[59:56.00] it won't capture the notions we need for
[59:57.76] formulation of curvature
[60:00.16] so it turns out to be impossible to
[60:01.68] define
[60:02.64] a natural the emphasis is on the word
[60:05.04] natural here
[60:14.08] uh this isn't such a problem but let's
[60:16.16] just explain what the problem is
[60:19.60] uh
[60:20.72] the problem is we want
[60:22.96] and this is absolutely fundamental to
[60:25.28] the notion of parallel transport we want
[60:27.60] to move
[60:30.80] a vector
[60:33.68] from say a tangent space
[60:36.08] v p at p
[60:38.80] to
[60:39.92] another tangent space v q at q
[60:44.96] and in as
[60:47.92] parallel
[60:49.76] away as possible
[60:56.40] and the problem is that there's no
[60:58.56] natural way to compare elements of vp
[61:01.36] and vq
[61:18.56] or
[61:19.52] even
[61:21.04] infinitesimally close by ones right so
[61:23.28] vp
[61:27.28] and vp plus dp whatever that means
[61:29.60] imagine if
[61:30.56] p plus dp is very very very close
[61:33.04] you know we don't even have the enough
[61:34.88] data to compare a vector one
[61:38.08] if you if you're one
[61:39.92] point in a
[61:41.36] manifold and a very very close by point
[61:44.72] we don't even have a way to compare two
[61:46.32] vectors two tangent vectors in these
[61:48.40] different
[61:49.68] tangent spaces
[61:51.68] turns out we need extra data right
[61:54.80] and that's okay right it's just it's
[61:56.16] just admitting that we need extra data
[61:58.08] before we can come up with a natural
[62:00.08] notion of parallel transport so in
[62:02.56] euclidean space this is where we always
[62:04.32] turn to for guidance in trying to
[62:06.48] formulate these notions because these
[62:08.48] notions um
[62:12.56] you know should all be solvable all
[62:13.92] these problems these issues should be
[62:15.36] solvable in euclidean space
[62:17.36] so what do we do well in rn we uh
[62:21.44] take a vector at p
[62:27.92] and shift it right we use the group
[62:30.24] structure of rm
[62:32.80] rn or additive structure
[62:43.84] to cube that was easy
[62:46.96] and then
[62:47.51] [Music]
[62:49.76] i note on the side that this allows us
[62:52.56] to define derivatives of vectors
[63:04.08] so suppose you have a vector v mu
[63:06.88] with respect to the coordinate bases at
[63:08.72] some um
[63:10.24] point p
[63:11.92] i'll just call it vp
[63:17.36] and and how would we define the
[63:18.80] derivative of this vector well we just
[63:20.56] do it with respect to these components
[63:30.56] just take the components of the vector
[63:33.44] evaluate them at a translated point
[63:36.88] the new so this is defining the partial
[63:39.04] derivative
[63:40.56] with respect to the
[63:42.48] newth coordinate
[63:48.40] and then divide by delta t delta x nu
[63:50.96] and then take the limit as delta x mu
[63:53.04] tends to zero
[63:57.20] so that's how
[63:58.64] we can use the additive structure to
[64:00.08] transport vectors and vector components
[64:02.08] around and further more define
[64:03.28] derivatives
[64:05.12] but you know if we were thinking in
[64:06.64] terms of tangent spaces this vector here
[64:10.96] is uh this vector's
[64:13.20] defined you know where is it well it
[64:15.76] certainly isn't defined with respect to
[64:17.52] the vector space at point p
[64:19.12] it's defined
[64:21.36] um at the vector space v
[64:24.16] q
[64:25.04] where q is equal to p plus well you know
[64:28.48] dp and dp in this case is the in
[64:30.72] coordinates
[64:31.84] delta x nu then youth component
[64:37.92] and
[64:38.64] the way we get away with it in euclidean
[64:40.40] space
[64:41.44] is that uh
[64:46.80] we assume that the parallel transported
[64:49.44] version
[64:52.08] we define parallel transport effectively
[64:54.08] implicitly here
[65:00.72] so we define the
[65:02.16] assumed parallel transporter version
[65:05.04] uh
[65:06.16] of
[65:07.12] v to q
[65:10.56] has the same components
[65:19.92] as v at p
[65:25.44] so we want to move vectors around we
[65:27.68] need to have a notion of parallel
[65:29.04] transport and in euclidean space we got
[65:32.00] away with it by assuming that these
[65:33.52] vectors in one coordinate basis they
[65:35.76] have the same components if you move
[65:37.20] them around that's what parallel
[65:38.48] transport means
[65:40.32] in euclidean space and that's somehow
[65:42.24] the motivation for
[65:45.60] following
[65:49.36] technology to describe parallel
[65:51.20] transport transport on
[65:56.08] manifolds
[66:00.00] now to build up the theory of parallel
[66:02.72] transport and curvature manifolds and
[66:04.88] first take a heuristic approach to just
[66:07.60] explain what kind of data we need to do
[66:09.84] this and then we'll instantiate it with
[66:13.20] in a specific way in this course using
[66:15.84] abstract index notation
[66:18.72] so it's clear that we need somehow
[66:20.72] additional data
[66:22.56] if you have a general manifold
[66:24.64] you don't you can't make reference to a
[66:27.28] parallel transport in a flat spacetime
[66:29.68] flat manifold anymore there's no
[66:31.20] additive structure that you can you can
[66:33.60] access we're going to try and find a
[66:35.44] proxy for that additive structure
[66:37.68] via something called a parallel
[66:39.92] transporter
[66:44.24] so this is extra machinery extra data
[66:46.32] that we don't have given to us when we
[66:47.92] specify a manifold so what is a parallel
[66:49.84] transporter well it's a machine
[66:53.84] it's a machine calligraphic u that
[66:55.60] depends on the path chosen that connects
[66:57.60] two points and it takes an entry in a
[67:01.04] vector space at p and gives you a vector
[67:03.92] in a vector space of q so here's the
[67:07.28] cartoon manifold
[67:09.92] here's p
[67:11.36] here's q
[67:13.36] here's some path chosen
[67:15.52] and here's a vector at p and a vector at
[67:17.52] q
[67:18.24] so we need very very the very very least
[67:21.04] we need
[67:22.48] a thing that can
[67:24.48] eat a vector at
[67:25.84] tangent space at p and give us hand us
[67:28.96] back a vector at q
[67:31.52] so that's the extra data that we need
[67:32.96] and we're going to call that data a
[67:34.16] parallel transporter
[67:36.80] i haven't told you how to get one of
[67:38.08] these or how to define one of these it's
[67:39.84] just making the extremely obvious
[67:41.68] statement that to do parallel
[67:44.08] transporter we need something that can
[67:45.52] parallel transport
[67:47.12] where
[67:48.16] what is gamma here gamma is a smooth
[67:50.24] path
[67:55.20] uh connecting p and q
[68:04.32] if someone came you know if you had the
[68:06.40] if you come with someone came to you and
[68:08.40] gave you a list of a manifold
[68:10.64] and then they gave you this parallel
[68:12.40] transporter machine for all paths gamma
[68:15.60] then you'd be done you you could um
[68:20.32] you could define a notion of parallel
[68:21.84] transport simply because someone defined
[68:23.52] it for you
[68:25.44] and if you had
[68:28.00] the data of a parallel transporter
[68:35.68] then you're in a good position to
[68:40.64] define other things in fact more more
[68:42.40] important things to do with curvature so
[68:44.16] if you had the data of a parallel
[68:45.28] transporter you gamma
[68:47.28] you could def you could compare tangent
[68:49.04] vectors
[68:50.32] obviously right um
[69:01.20] it's worth sort of saying this
[69:02.48] explicitly
[69:05.68] at
[69:06.56] p and
[69:08.56] q how would you do it well
[69:10.80] let
[69:12.80] v be a tangent vector
[69:15.12] point p
[69:16.40] let w be a tangent vector at point q
[69:20.56] then
[69:22.64] define
[69:25.68] v at
[69:27.12] q
[69:31.36] to be the vector u gamma applied to v
[69:35.84] okay
[69:36.64] that's what it would mean
[69:40.88] to parallel transport the vector v at
[69:43.28] point p to q
[69:48.08] the manifold here's the point p is the
[69:50.88] point
[69:51.76] q so suppose at the point p we have uh
[69:55.04] some vector like v
[69:57.12] suppose at the point q we have another
[69:58.96] vector i don't know w
[70:00.84] right now we have a path between them
[70:06.24] and u gamma is a box that that will
[70:08.88] accept
[70:09.84] um an argument at p and hand us an
[70:13.04] element at q and so the image of
[70:16.56] p under this u gamma box is this red
[70:19.76] vector here and you could then look at
[70:21.20] these two vectors w and and u gamma of v
[70:24.32] and sort of assess whether or not
[70:26.56] they're the same and that notion of
[70:29.04] if something's the same is intrinsic to
[70:31.12] or underlying
[70:32.56] uh
[70:34.48] any any uh
[70:36.08] abstract characterization of parallel
[70:38.24] transport parallel means keep you
[70:40.32] looking
[70:41.20] keep something the same pointing in the
[70:43.28] same direction so you need a notion of
[70:44.80] sameness
[70:45.92] so you could compare
[70:47.52] um
[70:48.80] u gamma of v and w now so given the data
[70:52.40] of a parallel transporter you have
[70:53.76] enough
[70:56.96] to compare
[70:59.84] vectors at different tangent spaces
[71:03.76] you brought v to w
[71:07.60] uh and so the interesting thing is you
[71:09.52] know so we have this uh
[71:13.68] this hypothetical parallel transporter
[71:15.60] box what do we want from it and then how
[71:18.16] can we specify one you know okay we
[71:20.32] don't have one but what do we need to
[71:21.68] specify one
[71:23.12] so we're going to make some demands and
[71:24.64] properties we're going to firstly demand
[71:26.64] that new gamma is a linear
[71:27.92] transformation
[71:39.12] okay so given the input of a path smooth
[71:41.28] path gamma we're going to insist that u
[71:43.60] gamma takes a vector and linear
[71:45.92] combinations to linear combinations fair
[71:47.68] enough
[71:48.64] i mean that we we could hope
[71:50.64] and indeed that turns out to be okay
[71:52.32] that we can insist on that
[71:55.52] okay but
[71:57.84] what do we really need
[72:02.16] to specify such a magical box
[72:05.92] well it turns out sort of surprisingly
[72:07.92] little is the answer
[72:09.52] at first sight you know at first sight
[72:11.12] it looks like you gamma depends on path
[72:13.44] chosen right so it looks like you gamma
[72:15.28] is some weird global uh depends on the
[72:17.68] global information so
[72:19.60] uh
[72:22.48] a u is a function
[72:24.48] you know
[72:25.36] u gamma just just to give you a sense of
[72:27.52] why why this is really hard
[72:29.36] you as a machine takes paths
[72:33.04] in m as one of its arguments
[72:35.52] it takes
[72:37.12] vectors
[72:38.56] um
[72:39.52] in some vector space vp and it takes it
[72:41.76] to some
[72:42.80] vector space vq so just the
[72:45.44] the
[72:46.16] infinite dimensionality of all the paths
[72:48.16] that can be in m makes you sort of
[72:49.84] suspect that maybe you gamma is a really
[72:51.68] complicated object
[72:53.44] well it turns out that's not the case
[72:55.20] and it's kind of amazing right because
[72:57.20] it
[72:58.72] in fact it it so it looks like you
[73:00.32] gammon you depends on the global
[73:02.24] information encoded in paths but it
[73:04.80] turns out it doesn't depend on global
[73:06.32] information in fact it can be defined
[73:07.92] infinitesimally and locally which is
[73:09.44] really kind of amazing
[73:12.64] so
[73:14.56] and to emphasize this or to illustrate
[73:16.56] this i'm gonna we're gonna work
[73:18.24] infinitesimally from for a little while
[73:20.16] now
[73:25.28] so let v be just some vector in a
[73:28.56] tangent space at a point p in the
[73:30.48] manifold
[73:32.96] and
[73:34.40] we're going to work in a coordinate
[73:36.16] chart
[73:38.16] so p suppose p itself has components
[73:44.40] x mu in a chart
[73:47.44] psi
[73:50.72] and then we're going to consider a
[73:51.84] nearby point infinitesimally close
[73:57.44] so consider q
[73:58.84] near p
[74:01.36] and not just near but really near
[74:03.32] infinitesimally close
[74:11.76] so what does that mean so
[74:14.64] in a chart or in the chart psi
[74:17.68] this means that
[74:19.28] psi of q
[74:22.40] the coordinates of q are basically
[74:25.76] only infinitesimally different from the
[74:27.60] coordinates of p
[74:32.24] so
[74:34.80] we're now in the same coordinate chart
[74:36.32] and we have infinitesimally closed
[74:37.76] points
[74:38.96] so let's
[74:41.52] introduce a path
[74:45.04] so let gamma be a smooth path
[74:53.36] connecting x and x plus
[74:56.48] sorry p and q right in the manifold
[75:02.00] and then we're going to consider the
[75:02.96] components of this path psi compose
[75:05.52] gamma
[75:07.44] so in the
[75:08.84] chart psi this path is
[75:11.76] first do the path then
[75:14.08] project down to the coordinate chart
[75:17.84] and we demand what do we demand right
[75:21.36] uh for a good parallel transporter we're
[75:23.60] going to have
[75:24.80] some
[75:25.68] some basic properties that we we expect
[75:28.00] from our parallel transporter
[75:30.64] so if indeed two two points are
[75:33.28] infinitesimally close
[75:35.04] then we demand that the components
[75:37.76] the twiddle
[75:39.92] of the parallel transported version of v
[75:46.24] they have to be
[75:47.52] somehow
[75:49.52] not completely insane right
[75:51.68] and so what's some key requirements that
[75:53.76] we would have of a parallel transported
[75:56.40] vector
[75:57.36] an infinitesimally parallel transported
[76:00.00] vector well
[76:01.36] demand one
[76:03.12] is that and i think this is this should
[76:05.44] be intuitively uh pretty reasonable
[76:09.04] the components of your parallel
[76:10.56] transported vector should be only
[76:12.24] infinitesimally different from the
[76:13.76] components of the vector at the original
[76:15.92] point
[76:18.80] right it should really
[76:20.72] depend infinite they should be
[76:22.24] proportional they shouldn't go like
[76:24.56] super big right if you move a vector
[76:26.40] only an infinitesimal distance then the
[76:28.32] ejector can't really change a lot
[76:31.44] and the second one is just the linear
[76:32.88] transformation property that we already
[76:34.56] demanded so if you if you parallel
[76:36.56] transport
[76:37.92] the
[76:39.76] superposition or sum of two vectors it
[76:41.92] better be the same as having parallel
[76:43.68] transport of the vectors separately
[76:47.52] so you know here's the picture just to
[76:49.20] have this
[76:50.32] a bit more concretely
[76:52.32] denoted so here's p and here's p plus
[76:55.52] delta p
[76:57.52] um here's v
[76:59.12] and we're in some kind of chart yeah so
[77:03.76] this is our chart psi and so in this
[77:06.24] chart we have p and we've got uh
[77:09.12] we've got the coordinates of p which is
[77:11.20] x
[77:12.00] and we've got x plus delta x right we're
[77:14.08] really close by these two points we have
[77:15.92] a vector with components
[77:18.08] v mu and we demand that you know
[77:20.64] whatever parallel transport does
[77:23.36] when you apply it when you parallel
[77:25.20] transport a vector infinitesimally to a
[77:27.52] point that's infinitesimally nearby
[77:30.72] it better be that the components of the
[77:32.56] new vector aren't that different from
[77:33.76] the original that's what condition one
[77:35.36] says
[77:36.32] and condition two just says that if you
[77:38.00] apply parallel transport to a linear
[77:40.24] combination of vectors it better be a
[77:41.44] linear combination of parallel
[77:42.64] transports
[77:45.12] now we can
[77:47.04] satisfy both these
[77:54.96] if we take
[77:58.00] the following prescription
[77:59.84] this is an exercise
[78:03.04] namely the components of the parallel
[78:04.80] infinitesimally parallel transported
[78:06.96] vector
[78:08.00] are related to the components of the
[78:10.24] original vector in this chart
[78:12.80] plus a linear combination
[78:15.12] of
[78:16.24] numbers that depend only on the
[78:19.04] displacement
[78:22.00] so this is i mean look
[78:23.84] this is an exercise if you take this
[78:32.00] prescription here for parallel
[78:33.44] infinitesimal parallel transport bear in
[78:35.84] mind right you know where it is
[78:38.00] where does v twiddle live well it's
[78:40.08] corresponding to um
[78:41.79] [Music]
[78:44.48] lives at
[78:45.60] q a psi of q which is x plus delta x
[78:51.68] and so what are these
[78:53.04] uh so we we're demanding that the
[78:55.36] infinitesimal parallel transport of a
[78:58.00] vector in one coordinate chart gives you
[79:00.16] the components of the original vector
[79:01.84] that's what these these were here the
[79:03.36] original components
[79:10.96] plus something of order epsilon of order
[79:13.20] delta x
[79:14.72] and it's got to be linear so that's why
[79:16.64] it's a linear transformation this second
[79:18.16] term here is linear so this is just a
[79:20.00] list of of numbers and these this list
[79:23.60] of numbers are called
[79:26.32] uh well perhaps better say list of
[79:30.08] uh
[79:31.04] functions these are these
[79:34.00] functions here that depend on the
[79:35.52] position in the manifold are called the
[79:36.80] connection coefficients
[79:41.28] so to satisfy these two requirements one
[79:43.12] and two of infinitesimal parallel
[79:44.96] transport really what do you need is you
[79:46.40] need a list of
[79:48.64] things called connection coefficients
[79:50.40] and how many are there well there's
[79:52.32] if we have n components this
[79:54.32] you can see that there's three indices
[79:55.84] here we're working components there's no
[79:57.68] abstract index notation at the moment
[79:59.28] we're in a coordinate chart you have n
[80:01.28] indices
[80:02.40] you have mu nu and lambda they all must
[80:04.40] be allowed to run over n indices so you
[80:06.00] have at most n cubed
[80:08.32] uh
[80:10.00] scalar functions that determine this
[80:11.76] this notion
[80:14.72] so
[80:15.76] we need
[80:18.56] these
[80:19.52] connection coefficients for each
[80:23.36] point
[80:24.48] x in the chart
[80:28.56] to build the notion of infinitesimal
[80:30.72] parallel transport
[80:32.56] now
[80:36.80] once you have that data
[80:38.48] i'm just going to state the the
[80:40.96] following and we'll justify this later
[80:43.36] once you have a notion of infinitesimal
[80:45.12] parallel transport you hereby have a
[80:46.96] notion of parallel transport because
[80:49.04] what is
[80:50.08] parallel transport over a long distance
[80:54.40] well you could just define it
[80:57.12] parallel transport over a long distance
[80:59.12] to be just a lot of little parallel
[81:00.72] transports infinitesimal ones so that's
[81:03.12] how indeed how we're going to define
[81:04.40] parallel transport in general
[81:07.68] now
[81:09.12] that we still have some hard work to do
[81:10.96] here um and that the reason we have hard
[81:13.68] work to do even you know if you believe
[81:15.76] this this this notion here is that it
[81:17.52] looks coordinate dependent
[81:20.80] and that's that's
[81:22.80] that's bad right in physics we don't
[81:24.24] want anything to depend on the
[81:25.36] coordinates we chose because if we
[81:26.80] choose spherical coordinates today and
[81:28.56] cartesian coordinates tomorrow if our
[81:30.08] answers depend on the coordinates then
[81:31.52] we've done something wrong
[81:33.76] um so
[81:37.60] so all this looks
[81:40.64] coordinate dependent
[81:43.12] so far right i had to choose a chart to
[81:45.20] write down this notion of parallel
[81:46.64] transport
[81:49.52] and what we're going to look for is to
[81:53.36] an intrinsic way
[82:03.44] so we've got to take one level of
[82:04.72] dereferencing one level of abstraction
[82:06.72] to do this and we note
[82:09.36] that to every infinitesimal notion of
[82:11.60] parallel transport we can introduce a
[82:13.20] kind of derivative operator
[82:16.72] so this is the key observation
[82:18.16] underlying the definition of parallel
[82:20.00] transport of general manifolds
[82:22.80] so
[82:23.52] to every uh
[82:25.84] in
[82:26.92] infinitesimal notion
[82:33.36] of parallel
[82:36.84] transport we can get a derivative
[82:39.60] operation
[82:44.00] of vectors
[82:47.44] you know because we're talking about uh
[82:48.88] vectors here the whole time
[82:53.60] so every time you have a parallel
[82:54.96] transporter you gamma you can define a
[82:56.72] derivative type operation and i'm going
[82:59.04] to give you the notation for this right
[83:01.12] so what is this derivative
[83:03.52] for
[83:04.84] vectors it's important to say it's for
[83:06.80] vectors and the notation i'm going to
[83:08.16] use for this derivative type operation
[83:09.76] is delta nu
[83:11.92] and delta nu takes some vector which in
[83:14.72] components
[83:18.40] is written like so and
[83:21.84] the way it works is you just form the
[83:24.16] usual derivative where you instead of um
[83:26.95] [Music]
[83:31.36] oops
[83:32.80] where you compare two vectors nearby but
[83:35.20] now you have a way to do that right via
[83:37.68] this parallel transport notion
[83:48.72] and in particular you compare them at
[83:50.40] the point q
[83:55.20] and then substitute and so
[83:58.08] right v is
[84:00.08] the vector at q
[84:02.64] that we're trying to compare with and v
[84:05.76] twiddle is the vector
[84:07.92] uh at p
[84:10.24] uh moved parallel by a parallel
[84:12.88] transport to q
[84:15.60] and just substituting in the definition
[84:17.36] we have up above so remember here's
[84:20.32] here's how we do infinitesimal parallel
[84:22.08] transport in this yellow box here
[84:24.72] substituting that into here gives us a
[84:26.88] really a complete a concrete formula for
[84:29.68] this derivative operation
[84:39.20] so this
[84:40.32] is how
[84:41.52] we should do a
[84:44.56] parallel transport
[84:47.36] this is how we can define a derivative
[84:48.96] operator corresponding to a parallel
[84:50.72] transport operation
[84:54.32] so now we have
[84:57.44] the key idea that we're going to use to
[84:59.28] abstract
[85:01.84] and build an intrinsic notion of
[85:03.52] parallel transport
[85:05.44] what we're really going to do and this
[85:07.04] is what we're going to commence in the
[85:08.16] next lecture is instead of talking
[85:10.80] building parallel transporter operations
[85:13.04] you gamma associated to manifolds
[85:16.16] instead of building these thingies
[85:18.80] as our notion of parallel transport
[85:20.64] instead we're going to go one level of
[85:22.40] abstraction further and say well
[85:24.72] corresponding to every parallel
[85:26.16] transport operation there is a
[85:27.12] derivative type operator delta nu
[85:29.92] how about we uh
[85:34.00] define
[85:35.04] delta derivative type operators on
[85:37.12] general manifolds and that's what we're
[85:38.40] going to do in the next lecture we're
[85:39.44] going to take
[85:40.80] as our
[85:42.08] the the basis for our notion of parallel
[85:43.84] transport that of derivative type
[85:45.84] operators and we're going to use
[85:46.88] derivative type operators
[85:48.72] as to build ourselves notions of
[85:50.96] infinitesimal parallel transport
[85:53.36] and it turns out that you know we've
[85:55.04] successfully used derivative type
[85:56.56] operations to abs to abstract notions
[85:59.04] earlier namely we use derivative
[86:00.48] operations to abstract the notion of a
[86:02.24] tangent vector
[86:04.56] and it turns out to be again a very
[86:06.24] successful strategy human will
[86:07.92] definitely employ in general relativity
[86:09.44] to use derivative type operators to
[86:10.80] abstract the notion of parallel
[86:11.92] transport as well
[86:13.60] so that's where we'll end for today
[86:15.60] thank you very much for your attention
[86:16.96] and see you next time
